{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "611aa6db-9102-41d8-a575-6da09450a2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import _pickle as pickle\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import timeit\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "from utils import set_size, pgf_with_latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9bd8093",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"seaborn-v0_8-colorblind\")\n",
    "# But with fonts from the document body\n",
    "plt.rcParams.update(pgf_with_latex)\n",
    "\n",
    "# Using the set_size function as defined earlier\n",
    "doc_width_pt = 452.9679"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1b7a68a-60d0-4758-b3f9-b5ef665b2722",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_eval_name = {\n",
    "    \"pretraining\": \"\",\n",
    "    \"pretrain-sample_high_prob_class_only-start_pos_0\": \"Condition on High Frequency\",\n",
    "    \"pretrain-sample_low_prob_class_only-start_pos_0\": \"Condition on Low Frequency\",\n",
    "    \"pretrain-sample_high_prob_class_only-start_pos_0-flip_label\": \"Condition on High Frequency\",\n",
    "    \"pretrain-sample_low_prob_class_only-start_pos_0-flip_label\": \"Condition on Low Frequency\",\n",
    "    \n",
    "    \"pretrain-sample_high_prob_class_only-start_pos_1\": \"Condition on High Frequency\",\n",
    "    \"pretrain-sample_low_prob_class_only-start_pos_1\": \"Condition on Low Frequency\",\n",
    "    \"pretrain-sample_high_prob_class_only-start_pos_1-flip_label\": \"Condition on High Frequency\",\n",
    "    \"pretrain-sample_low_prob_class_only-start_pos_1-flip_label\": \"Condition on Low Frequency\",\n",
    "    \n",
    "    \"pretrain-sample_high_prob_class_only-start_pos_7\": \"Condition on High Frequency\",\n",
    "    \"pretrain-sample_low_prob_class_only-start_pos_7\": \"Condition on Low Frequency\",\n",
    "    \"pretrain-sample_high_prob_class_only-start_pos_7-flip_label\": \"Condition on High Frequency\",\n",
    "    \"pretrain-sample_low_prob_class_only-start_pos_7-flip_label\": \"Condition on Low Frequency\",\n",
    "\n",
    "    \"eval-relevant_context-none\": \"Relevant Context\",\n",
    "    \"eval-irrelevant_context-none\": \"Irrelevant Context\",\n",
    "}\n",
    "\n",
    "stats_keys = [\n",
    "    \"accuracy\",\n",
    "    \"p_iwl\",\n",
    "    \"context contains query class\",\n",
    "    \"loss\",\n",
    "    \"ic_pred\",\n",
    "    \"iw_pred\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dc71aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path = \"/Users/chanb/research/ualberta/icl/simple_icl\"\n",
    "results_dir = \"/Users/chanb/research/ualberta/icl/cc_results/paper_experiments/evaluation_results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e5fe6a4-d543-4e4e-9857-94bb492adc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_name = \"synthetic-transformer-noisy_inputs\"\n",
    "\n",
    "results_dir = \"/Users/chanb/research/ualberta/icl/simple_icl/experiments/test_convergence_eval\"\n",
    "variant_name = \"synthetic-transformer-input_noise-longer_epoch\"\n",
    "\n",
    "checkpoint_steps = 1000\n",
    "# checkpoint_steps = 500\n",
    "\n",
    "stats_file = os.path.join(repo_path, \"plot_utils/plots/agg_stats\", \"{}.feather\".format(variant_name))\n",
    "stats = pd.read_feather(stats_file)\n",
    "\n",
    "os.makedirs(\n",
    "    os.path.join(repo_path, \"plot_utils/plots/acc-plots\", variant_name),\n",
    "    exist_ok=True\n",
    ")\n",
    "\n",
    "results_dir = os.path.join(results_dir, variant_name)\n",
    "eval_namess = [\n",
    "    [\n",
    "        \"pretraining\"\n",
    "    ],\n",
    "    [\n",
    "        \"eval-relevant_context-none\",\n",
    "        \"eval-irrelevant_context-none\",\n",
    "    ],\n",
    "    [\n",
    "        \"pretrain-sample_high_prob_class_only-start_pos_0\",\n",
    "        \"pretrain-sample_low_prob_class_only-start_pos_0\",\n",
    "    ],\n",
    "    [\n",
    "        \"pretrain-sample_high_prob_class_only-start_pos_0-flip_label\",\n",
    "        \"pretrain-sample_low_prob_class_only-start_pos_0-flip_label\",\n",
    "    ],\n",
    "    [\n",
    "        \"pretrain-sample_high_prob_class_only-start_pos_1\",\n",
    "        \"pretrain-sample_low_prob_class_only-start_pos_1\",\n",
    "    ],\n",
    "    [\n",
    "        \"pretrain-sample_high_prob_class_only-start_pos_1-flip_label\",\n",
    "        \"pretrain-sample_low_prob_class_only-start_pos_1-flip_label\",\n",
    "    ],\n",
    "    [\n",
    "        \"pretrain-sample_high_prob_class_only-start_pos_7\",\n",
    "        \"pretrain-sample_low_prob_class_only-start_pos_7\",\n",
    "    ],\n",
    "    [\n",
    "        \"pretrain-sample_high_prob_class_only-start_pos_7-flip_label\",\n",
    "        \"pretrain-sample_low_prob_class_only-start_pos_7-flip_label\",\n",
    "    ],\n",
    "]\n",
    "plot_names = [\n",
    "    \"pretraining\",\n",
    "    \"context-default\",\n",
    "    \"iwl\",\n",
    "    \"iwl-flip_label\",\n",
    "    \"icl-last_context\",\n",
    "    \"icl-last_context-flip_label\",\n",
    "    \"icl-except_first_context\",\n",
    "    \"icl-except_first_context-flip_label\",\n",
    "]\n",
    "plot_titles = [\n",
    "    \"Pretraining\",\n",
    "    \"Context Default\"\n",
    "    \"In-weight Evaluation\",\n",
    "    \"In-weight Evaluation with Flipped Label\",\n",
    "    \"In-context Evaluation with Last Context\",\n",
    "    \"In-context Evaluation with Last Context + Flipped Label\",\n",
    "    \"In-context Evaluation with Contexts but First\",\n",
    "    \"In-context Evaluation with Contexts but First + Flipped Label\",\n",
    "]\n",
    "\n",
    "map_stats_key = {\n",
    "    \"p_iwl\": \"$\\\\alpha(x)$\",\n",
    "    \"accuracy\": \"Accuracy\",\n",
    "    \"context contains query class\": \"Context Contains Query Class\",\n",
    "    \"loss\": \"Loss\",\n",
    "    \"ic_pred\": \"In-context Accuracy\",\n",
    "    \"iw_pred\": \"In-weight Accuracy\"\n",
    "}\n",
    "\n",
    "map_variant = {\n",
    "    \"ground_truth_prob\": \"$P(g(x) = c)$\",\n",
    "    \"high_prob\": \"$P(high\\_freq.)$\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f9f88e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "variants = stats[\"variant\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040a7140",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d095e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecd4272",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(stats[\n",
    "    (stats[\"dataset_size\"] == \"16384\")\n",
    "    & (stats[\"stats_key\"] == \"loss\")\n",
    "    & (stats[\"eval_name\"] == \"pretraining\")\n",
    "    & (stats[\"input_noise_std\"] == \"0.4\")\n",
    "    & (stats[\"p_relevant_context\"] == \"0.0\")\n",
    "][\"stats\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c48806",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(np.array(stats[\n",
    "    (stats[\"dataset_size\"] == \"16384\")\n",
    "    & (stats[\"stats_key\"] == \"loss\")\n",
    "    & (stats[\"eval_name\"] == \"pretraining\")\n",
    "    & (stats[\"input_noise_std\"] == \"0.4\")\n",
    "    & (stats[\"p_relevant_context\"] == \"0.0\")\n",
    "][\"stats\"].to_list()), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "985cbd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sizes = sorted([int(dataset_size) for dataset_size in stats[\"dataset_size\"].unique()])\n",
    "input_noise_stds = sorted([float(input_noise_std) for input_noise_std in stats[\"input_noise_std\"].unique()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dfab1b",
   "metadata": {},
   "source": [
    "# Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bb961e",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d90e2219",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_idxes = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea60e9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_label = {\n",
    "    0.0: \"IW Predictor\",\n",
    "    1.0: \"IC Predictor\",\n",
    "    0.9: \"Transformer\",\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(8, 3))\n",
    "for ax_i, p_relevant_context in enumerate([0.0, 1.0]):\n",
    "    ax = axes[ax_i]\n",
    "    for variant_i, input_noise_std in enumerate(input_noise_stds):\n",
    "        losses_mean = []\n",
    "        losses_std = []\n",
    "\n",
    "        for dataset_size in dataset_sizes:\n",
    "            data = (np.array(stats[\n",
    "                (stats[\"p_relevant_context\"] == f\"{p_relevant_context}\")\n",
    "                & (stats[\"input_noise_std\"] == f\"{input_noise_std}\")\n",
    "                & (stats[\"stats_key\"] == \"loss\")\n",
    "                & (stats[\"eval_name\"] == \"pretraining\")\n",
    "                & (stats[\"dataset_size\"] == f\"{dataset_size}\")\n",
    "            ][\"stats\"].to_list()))\n",
    "            best_idx = data.shape[1] - np.argmin(data[:, ::-1], axis=-1) - 1\n",
    "            sample = data[np.arange(5), best_idx]\n",
    "            curr_mean = np.mean(sample)\n",
    "            curr_std = np.std(sample)\n",
    "            \n",
    "            losses_mean.append(curr_mean)\n",
    "            losses_std.append(curr_std)\n",
    "\n",
    "        losses_mean = np.array(losses_mean)\n",
    "        losses_std = np.array(losses_std) / np.sqrt(5)\n",
    "        print(losses_mean)\n",
    "\n",
    "        ax.scatter(np.log2(np.array(dataset_sizes)), losses_mean, label=input_noise_std if ax_i == 0 else \"\")\n",
    "        # ax.fill_between(np.log2(np.array(dataset_sizes)), losses_mean - losses_std, losses_mean + losses_std, alpha=0.3)\n",
    "\n",
    "    ax.set_title(map_label[p_relevant_context])\n",
    "\n",
    "fig.supylabel(\"Cross-entropy Loss\")\n",
    "fig.supxlabel(\"Dataset Size (in $\\\\log_2$)\")\n",
    "fig.legend(\n",
    "    bbox_to_anchor=(0.0, 1.0, 1.0, 0.0),\n",
    "    loc=\"lower center\",\n",
    "    ncols=4,\n",
    "    borderaxespad=0.0,\n",
    "    frameon=True,\n",
    "    fontsize=\"8\", \n",
    ")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec7c64d",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983ec8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_label = {\n",
    "    0.0: \"IW Predictor\",\n",
    "    1.0: \"IC Predictor\",\n",
    "    0.9: \"Transformer\",\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(8, 3))\n",
    "for ax_i, p_relevant_context in enumerate([0.0, 1.0, 0.9]):\n",
    "    ax = axes[ax_i]\n",
    "    for variant_i, input_noise_std in enumerate(input_noise_stds):\n",
    "        losses_mean = []\n",
    "        losses_std = []\n",
    "\n",
    "        for dataset_size in dataset_sizes:\n",
    "            data = 1 - (np.array(stats[\n",
    "                (stats[\"p_relevant_context\"] == f\"{p_relevant_context}\")\n",
    "                & (stats[\"input_noise_std\"] == f\"{input_noise_std}\")\n",
    "                & (stats[\"stats_key\"] == \"accuracy\")\n",
    "                & (stats[\"eval_name\"] == \"pretraining\")\n",
    "                & (stats[\"dataset_size\"] == f\"{dataset_size}\")\n",
    "            ][\"stats\"].to_list()))\n",
    "            losses_mean.append(np.mean(data, axis=0)[-1])\n",
    "            losses_std.append(np.std(data, axis=0)[-1])\n",
    "\n",
    "        losses_mean = np.array(losses_mean)\n",
    "        losses_std = np.array(losses_std) / np.sqrt(5)\n",
    "\n",
    "        ax.plot(np.log2(np.array(dataset_sizes)), losses_mean, label=input_noise_std if ax_i == 0 else \"\")\n",
    "        ax.fill_between(np.log2(np.array(dataset_sizes)), losses_mean - losses_std, losses_mean + losses_std, alpha=0.3)\n",
    "\n",
    "    ax.set_title(map_label[p_relevant_context])\n",
    "    ax.set_ylim(0.0, 1.1)\n",
    "    ax.axhline(9/10, label=\"Chance\" if ax_i == 0 else \"\", c=\"red\", linestyle=\"--\")\n",
    "\n",
    "fig.supylabel(\"0-1 Error\")\n",
    "fig.supxlabel(\"Dataset Size (in $\\\\log_2$)\")\n",
    "fig.legend(\n",
    "    bbox_to_anchor=(0.0, 1.0, 1.0, 0.0),\n",
    "    loc=\"lower center\",\n",
    "    ncols=5,\n",
    "    borderaxespad=0.0,\n",
    "    frameon=True,\n",
    "    fontsize=\"8\", \n",
    ")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789213c5",
   "metadata": {},
   "source": [
    "## General Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6989a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_label = {\n",
    "    0.0: \"IW Predictor\",\n",
    "    1.0: \"IC Predictor\",\n",
    "    0.9: \"Transformer\",\n",
    "}\n",
    "map_eval = {\n",
    "    \"eval-relevant_context-high_prob\": \"Rel. Context, High-freq.\",\n",
    "    \"eval-relevant_context-low_prob\": \"Rel. Context, Low-freq.\",\n",
    "    \"eval-irrelevant_context-high_prob\": \"Irrel. Context, High-freq.\",\n",
    "    \"eval-irrelevant_context-low_prob\": \"Irrel. Context, Low-freq.\",\n",
    "    \"eval-relevant_context-high_prob-flip_label\": \"Rel. Context, High-freq.\",\n",
    "    \"eval-relevant_context-low_prob-flip_label\": \"Rel. Context, Low-freq.\",\n",
    "    \"eval-irrelevant_context-high_prob-flip_label\": \"Irrel. Context, High-freq.\",\n",
    "    \"eval-irrelevant_context-low_prob-flip_label\": \"Irrel. Context, Low-freq.\",\n",
    "}\n",
    "\n",
    "eval_namess = [\n",
    "    [\n",
    "        \"eval-relevant_context-high_prob\",\n",
    "        \"eval-relevant_context-low_prob\",\n",
    "        \"eval-irrelevant_context-high_prob\",\n",
    "        \"eval-irrelevant_context-low_prob\",\n",
    "    ],\n",
    "    [\n",
    "        \"eval-relevant_context-high_prob-flip_label\",\n",
    "        \"eval-relevant_context-low_prob-flip_label\",\n",
    "        \"eval-irrelevant_context-high_prob-flip_label\",\n",
    "        \"eval-irrelevant_context-low_prob-flip_label\",\n",
    "    ],\n",
    "]\n",
    "\n",
    "for input_noise_std in input_noise_stds:\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(10, 5))\n",
    "    for row_i, eval_names in enumerate(eval_namess):\n",
    "        for eval_i, eval_name in enumerate(eval_names):\n",
    "            ax = axes[row_i, eval_i]\n",
    "            # ax = axes[eval_i]\n",
    "            for p_relevant_context in [0.0, 0.9, 1.0]:\n",
    "                losses_mean = []\n",
    "                losses_std = []\n",
    "\n",
    "                for dataset_size in dataset_sizes:\n",
    "                    data = (np.array(stats[\n",
    "                        (stats[\"p_relevant_context\"] == f\"{p_relevant_context}\")\n",
    "                        & (stats[\"input_noise_std\"] == f\"{input_noise_std}\")\n",
    "                        & (stats[\"stats_key\"] == \"loss\")\n",
    "                        & (stats[\"eval_name\"] == eval_name)\n",
    "                        & (stats[\"dataset_size\"] == f\"{dataset_size}\")\n",
    "                    ][\"stats\"].to_list()))\n",
    "                    losses_mean.append(np.mean(data, axis=0)[-1])\n",
    "                    losses_std.append(np.std(data, axis=0)[-1])\n",
    "\n",
    "                losses_mean = np.array(losses_mean)\n",
    "                losses_std = np.array(losses_std) / np.sqrt(5)\n",
    "\n",
    "                ax.plot(np.log2(np.array(dataset_sizes)), losses_mean, label=map_label[p_relevant_context] if eval_i == 0 and row_i == 0 else \"\")\n",
    "                ax.fill_between(np.log2(np.array(dataset_sizes)), losses_mean - losses_std, losses_mean + losses_std, alpha=0.3)\n",
    "\n",
    "            ax.set_title(map_eval[eval_name])\n",
    "            # ax.set_ylim(-1.0, 1.5)\n",
    "            if eval_i == 0:\n",
    "                ax.set_ylabel(\"In-base Dist.\" if row_i == 0 else \"Out-of-base Dist.\")\n",
    "\n",
    "    fig.suptitle(\"Input Noise: {}\".format(input_noise_std))\n",
    "    fig.supylabel(\"Cross-entropy Loss\")\n",
    "    fig.supxlabel(\"Dataset Size (in $\\\\log_2$)\")\n",
    "    fig.legend(\n",
    "        bbox_to_anchor=(0.0, 1.0, 1.0, 0.0),\n",
    "        loc=\"lower center\",\n",
    "        ncols=4,\n",
    "        borderaxespad=0.0,\n",
    "        frameon=True,\n",
    "        fontsize=\"8\", \n",
    "    )\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd7b28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_label = {\n",
    "    0.0: \"IW Predictor\",\n",
    "    1.0: \"IC Predictor\",\n",
    "    0.9: \"Transformer\",\n",
    "}\n",
    "map_eval = {\n",
    "    \"eval-relevant_context-high_prob\": \"Rel. Context, High-freq.\",\n",
    "    \"eval-relevant_context-low_prob\": \"Rel. Context, Low-freq.\",\n",
    "    \"eval-irrelevant_context-high_prob\": \"Irrel. Context, High-freq.\",\n",
    "    \"eval-irrelevant_context-low_prob\": \"Irrel. Context, Low-freq.\",\n",
    "    \"eval-relevant_context-high_prob-flip_label\": \"Rel. Context, High-freq.\",\n",
    "    \"eval-relevant_context-low_prob-flip_label\": \"Rel. Context, Low-freq.\",\n",
    "    \"eval-irrelevant_context-high_prob-flip_label\": \"Irrel. Context, High-freq.\",\n",
    "    \"eval-irrelevant_context-low_prob-flip_label\": \"Irrel. Context, Low-freq.\",\n",
    "}\n",
    "\n",
    "eval_namess = [\n",
    "    [\n",
    "        \"eval-relevant_context-high_prob\",\n",
    "        \"eval-relevant_context-low_prob\",\n",
    "        \"eval-irrelevant_context-high_prob\",\n",
    "        \"eval-irrelevant_context-low_prob\",\n",
    "    ],\n",
    "    [\n",
    "        \"eval-relevant_context-high_prob-flip_label\",\n",
    "        \"eval-relevant_context-low_prob-flip_label\",\n",
    "        \"eval-irrelevant_context-high_prob-flip_label\",\n",
    "        \"eval-irrelevant_context-low_prob-flip_label\",\n",
    "    ],\n",
    "]\n",
    "for input_noise_std in input_noise_stds:\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(10, 5))\n",
    "    for row_i, eval_names in enumerate(eval_namess):\n",
    "        for eval_i, eval_name in enumerate(eval_names):\n",
    "            ax = axes[row_i, eval_i]\n",
    "            # ax = axes[eval_i]\n",
    "            for p_relevant_context in [0.0, 0.9, 1.0]:\n",
    "                losses_mean = []\n",
    "                losses_std = []\n",
    "\n",
    "                for dataset_size in dataset_sizes:\n",
    "                    data = (np.array(stats[\n",
    "                        (stats[\"p_relevant_context\"] == f\"{p_relevant_context}\")\n",
    "                        & (stats[\"input_noise_std\"] == f\"{input_noise_std}\")\n",
    "                        & (stats[\"stats_key\"] == \"accuracy\")\n",
    "                        & (stats[\"eval_name\"] == eval_name)\n",
    "                        & (stats[\"dataset_size\"] == f\"{dataset_size}\")\n",
    "                    ][\"stats\"].to_list()))\n",
    "                    losses_mean.append(np.mean(data, axis=0)[-1])\n",
    "                    losses_std.append(np.std(data, axis=0)[-1])\n",
    "\n",
    "                losses_mean = np.array(losses_mean)\n",
    "                losses_std = np.array(losses_std) / np.sqrt(5)\n",
    "\n",
    "                ax.plot(np.log2(np.array(dataset_sizes)), losses_mean, label=map_label[p_relevant_context] if eval_i == 0 and row_i == 0 else \"\")\n",
    "                ax.fill_between(np.log2(np.array(dataset_sizes)), losses_mean - losses_std, losses_mean + losses_std, alpha=0.3)\n",
    "\n",
    "            ax.set_title(map_eval[eval_name])\n",
    "            ax.axhline(1/10, label=\"Chance\" if eval_i == 0 and row_i == 0 else \"\", c=\"red\", linestyle=\"--\")\n",
    "            ax.set_ylim(-0.1, 1.1)\n",
    "\n",
    "            if eval_i == 0:\n",
    "                ax.set_ylabel(\"In-base Dist.\" if row_i == 0 else \"Out-of-base Dist.\")\n",
    "\n",
    "    fig.supylabel(\"Accuracy\")\n",
    "    fig.supxlabel(\"Dataset Size (in $\\\\log_2$)\")\n",
    "    fig.legend(\n",
    "        bbox_to_anchor=(0.0, 1.0, 1.0, 0.0),\n",
    "        loc=\"lower center\",\n",
    "        ncols=4,\n",
    "        borderaxespad=0.0,\n",
    "        frameon=True,\n",
    "        fontsize=\"8\", \n",
    "    )\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaceadba",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats[\"eval_name\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff20de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_label = {\n",
    "    0.0: \"IW Predictor\",\n",
    "    1.0: \"IC Predictor\",\n",
    "    0.9: \"Transformer\",\n",
    "}\n",
    "map_eval = {\n",
    "    \"eval-relevant_context-high_prob\": \"Rel. Context, High-freq.\",\n",
    "    \"eval-relevant_context-low_prob\": \"Rel. Context, Low-freq.\",\n",
    "    \"eval-irrelevant_context-high_prob\": \"Irrel. Context, High-freq.\",\n",
    "    \"eval-irrelevant_context-low_prob\": \"Irrel. Context, Low-freq.\",\n",
    "    \"eval-relevant_context-high_prob-flip_label\": \"Rel. Context, High-freq.\",\n",
    "    \"eval-relevant_context-low_prob-flip_label\": \"Rel. Context, Low-freq.\",\n",
    "    \"eval-irrelevant_context-high_prob-flip_label\": \"Irrel. Context, High-freq.\",\n",
    "    \"eval-irrelevant_context-low_prob-flip_label\": \"Irrel. Context, Low-freq.\",\n",
    "    \"eval-relevant_context-none\": \"Rel. Context, IBD\",\n",
    "    \"eval-relevant_context-none-flip_label\": \"Rel. Context, OOBD\",\n",
    "    \"eval-irrelevant_context-none\": \"Irrel. Context, High-freq., IBD\",\n",
    "    \"eval-irrelevant_context-none-flip_label\": \"Irrel. Context, OOBD\",\n",
    "}\n",
    "\n",
    "eval_names = [\n",
    "    \"eval-relevant_context-none\",\n",
    "    \"eval-irrelevant_context-none\",\n",
    "    \"eval-relevant_context-none-flip_label\",\n",
    "    \"eval-irrelevant_context-none-flip_label\",\n",
    "]\n",
    "for input_noise_std in input_noise_stds:\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(10, 3))\n",
    "    for eval_i, eval_name in enumerate(eval_names):\n",
    "        ax = axes[eval_i]\n",
    "        for p_relevant_context in [0.0, 0.9, 1.0]:\n",
    "            losses_mean = []\n",
    "            losses_std = []\n",
    "\n",
    "            for dataset_size in dataset_sizes:\n",
    "                data = (np.array(stats[\n",
    "                    (stats[\"p_relevant_context\"] == f\"{p_relevant_context}\")\n",
    "                    & (stats[\"input_noise_std\"] == f\"{input_noise_std}\")\n",
    "                    & (stats[\"stats_key\"] == \"accuracy\")\n",
    "                    & (stats[\"eval_name\"] == eval_name)\n",
    "                    & (stats[\"dataset_size\"] == f\"{dataset_size}\")\n",
    "                ][\"stats\"].to_list()))\n",
    "                losses_mean.append(np.mean(data, axis=0)[-1])\n",
    "                losses_std.append(np.std(data, axis=0)[-1])\n",
    "\n",
    "            losses_mean = np.array(losses_mean)\n",
    "            losses_std = np.array(losses_std) / np.sqrt(5)\n",
    "\n",
    "            ax.plot(np.log2(np.array(dataset_sizes)), losses_mean, label=map_label[p_relevant_context] if eval_i == 0 else \"\")\n",
    "            ax.fill_between(np.log2(np.array(dataset_sizes)), losses_mean - losses_std, losses_mean + losses_std, alpha=0.3)\n",
    "\n",
    "        ax.set_title(map_eval[eval_name])\n",
    "        ax.axhline(1/10, label=\"Chance\" if eval_i == 0 else \"\", c=\"red\", linestyle=\"--\")\n",
    "        ax.set_ylim(-0.1, 1.1)\n",
    "\n",
    "    fig.suptitle(\"Input Noise: {}\".format(input_noise_std))\n",
    "    fig.supylabel(\"Accuracy\")\n",
    "    fig.supxlabel(\"Dataset Size (in $\\\\log_2$)\")\n",
    "    fig.legend(\n",
    "        bbox_to_anchor=(0.0, 1.0, 1.0, 0.0),\n",
    "        loc=\"lower center\",\n",
    "        ncols=4,\n",
    "        borderaxespad=0.0,\n",
    "        frameon=True,\n",
    "        fontsize=\"8\", \n",
    "    )\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd4c8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_label = {\n",
    "    0.0: \"IW Predictor\",\n",
    "    1.0: \"IC Predictor\",\n",
    "    0.9: \"Transformer\",\n",
    "}\n",
    "map_eval = {\n",
    "    \"eval-relevant_context-high_prob\": \"Rel. Context, High-freq.\",\n",
    "    \"eval-relevant_context-low_prob\": \"Rel. Context, Low-freq.\",\n",
    "    \"eval-irrelevant_context-high_prob\": \"Irrel. Context, High-freq.\",\n",
    "    \"eval-irrelevant_context-low_prob\": \"Irrel. Context, Low-freq.\",\n",
    "    \"eval-relevant_context-high_prob-flip_label\": \"Rel. Context, High-freq.\",\n",
    "    \"eval-relevant_context-low_prob-flip_label\": \"Rel. Context, Low-freq.\",\n",
    "    \"eval-irrelevant_context-high_prob-flip_label\": \"Irrel. Context, High-freq.\",\n",
    "    \"eval-irrelevant_context-low_prob-flip_label\": \"Irrel. Context, Low-freq.\",\n",
    "    \"eval-relevant_context-none\": \"Rel. Context, IBD\",\n",
    "    \"eval-relevant_context-none-flip_label\": \"Rel. Context, OOBD\",\n",
    "    \"eval-irrelevant_context-none\": \"Irrel. Context, High-freq., IBD\",\n",
    "    \"eval-irrelevant_context-none-flip_label\": \"Irrel. Context, OOBD\",\n",
    "    \"eval-default-none\": \"In-base Dist.\",\n",
    "    \"eval-default-none-flip_label\": \"Out-of-base Dist.\",\n",
    "}\n",
    "\n",
    "eval_names = [\n",
    "    \"eval-default-none\",\n",
    "    \"eval-default-none-flip_label\",\n",
    "]\n",
    "\n",
    "num_rows = 2\n",
    "num_cols = len(input_noise_stds)\n",
    "fig, axes = plt.subplots(\n",
    "    num_rows,\n",
    "    num_cols,\n",
    "    figsize=set_size(doc_width_pt, 0.95, (num_rows, num_cols), use_golden_ratio=False),\n",
    "    layout=\"constrained\",\n",
    ")\n",
    "for col_i, input_noise_std in enumerate(input_noise_stds):\n",
    "    for eval_i, eval_name in enumerate(eval_names):\n",
    "        ax = axes[eval_i, col_i]\n",
    "        for p_relevant_context in [0.0, 0.9, 1.0]:\n",
    "            losses_mean = []\n",
    "            losses_std = []\n",
    "\n",
    "            for dataset_size in dataset_sizes:\n",
    "                data = 1 - (np.array(stats[\n",
    "                    (stats[\"p_relevant_context\"] == f\"{p_relevant_context}\")\n",
    "                    & (stats[\"input_noise_std\"] == f\"{input_noise_std}\")\n",
    "                    & (stats[\"stats_key\"] == \"accuracy\")\n",
    "                    & (stats[\"eval_name\"] == eval_name)\n",
    "                    & (stats[\"dataset_size\"] == f\"{dataset_size}\")\n",
    "                ][\"stats\"].to_list()))\n",
    "                losses_mean.append(np.mean(data, axis=0)[-1])\n",
    "                losses_std.append(np.std(data, axis=0)[-1])\n",
    "\n",
    "            losses_mean = np.array(losses_mean)\n",
    "            losses_std = np.array(losses_std) / np.sqrt(5)\n",
    "\n",
    "            ax.plot(np.log2(np.array(dataset_sizes)), losses_mean, label=map_label[p_relevant_context] if eval_i == 0 and col_i == 0 else \"\")\n",
    "            ax.fill_between(np.log2(np.array(dataset_sizes)), losses_mean - losses_std, losses_mean + losses_std, alpha=0.3)\n",
    "\n",
    "        ax.axhline(9/10, label=\"Chance\" if eval_i == 0 and col_i == 0 else \"\", c=\"black\", linestyle=\"--\")\n",
    "        ax.set_ylim(-0.1, 1.1)\n",
    "\n",
    "        if col_i == 0:\n",
    "            ax.set_ylabel(map_eval[eval_name], fontsize=\"8\")\n",
    "    \n",
    "    axes[0, col_i].set_title(f\"$\\\\sigma = {input_noise_std}$\", fontsize=\"8\")\n",
    "\n",
    "# fig.suptitle(\"Input Noise\")\n",
    "fig.supylabel(\"0-1 Error\", fontsize=\"8\")\n",
    "fig.supxlabel(\"Dataset Size (in $\\\\log_2$)\", fontsize=\"8\")\n",
    "fig.legend(\n",
    "    bbox_to_anchor=(0.0, 1.0, 1.0, 0.0),\n",
    "    loc=\"lower center\",\n",
    "    ncols=4,\n",
    "    borderaxespad=0.0,\n",
    "    frameon=True,\n",
    "    fontsize=\"8\", \n",
    ")\n",
    "# fig.tight_layout()\n",
    "plt.savefig(\"noisy_inputs.pdf\", dpi=600, format=\"pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ab4e1d",
   "metadata": {},
   "source": [
    "# Num grad steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894d5b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_label = {\n",
    "    0.0: \"IW Predictor\",\n",
    "    1.0: \"IC Predictor\",\n",
    "    0.9: \"Transformer\",\n",
    "}\n",
    "map_eval = {\n",
    "    \"eval-relevant_context-high_prob\": \"Rel. Context, High-freq.\",\n",
    "    \"eval-relevant_context-low_prob\": \"Rel. Context, Low-freq.\",\n",
    "    \"eval-irrelevant_context-high_prob\": \"Irrel. Context, High-freq.\",\n",
    "    \"eval-irrelevant_context-low_prob\": \"Irrel. Context, Low-freq.\",\n",
    "    \"eval-relevant_context-high_prob-flip_label\": \"Rel. Context, High-freq.\",\n",
    "    \"eval-relevant_context-low_prob-flip_label\": \"Rel. Context, Low-freq.\",\n",
    "    \"eval-irrelevant_context-high_prob-flip_label\": \"Irrel. Context, High-freq.\",\n",
    "    \"eval-irrelevant_context-low_prob-flip_label\": \"Irrel. Context, Low-freq.\",\n",
    "    \"eval-relevant_context-none\": \"Rel. Context, IBD\",\n",
    "    \"eval-relevant_context-none-flip_label\": \"Rel. Context, OOBD\",\n",
    "    \"eval-irrelevant_context-none\": \"Irrel. Context, IBD\",\n",
    "    \"eval-irrelevant_context-none-flip_label\": \"Irrel. Context, OOBD\",\n",
    "}\n",
    "\n",
    "eval_namess = [\n",
    "    [\n",
    "        \"eval-relevant_context-high_prob\",\n",
    "        \"eval-relevant_context-low_prob\",\n",
    "        \"eval-irrelevant_context-high_prob\",\n",
    "        \"eval-irrelevant_context-low_prob\",\n",
    "    ],\n",
    "    [\n",
    "        \"eval-relevant_context-high_prob-flip_label\",\n",
    "        \"eval-relevant_context-low_prob-flip_label\",\n",
    "        \"eval-irrelevant_context-high_prob-flip_label\",\n",
    "        \"eval-irrelevant_context-low_prob-flip_label\",\n",
    "    ],\n",
    "]\n",
    "\n",
    "\n",
    "input_noise_std = 0.4\n",
    "dataset_size = 16384\n",
    "checkpoint_interval = 500\n",
    "\n",
    "num_rows = 2\n",
    "num_cols = 4\n",
    "fig, axes = plt.subplots(\n",
    "    num_rows,\n",
    "    num_cols,\n",
    "    figsize=set_size(doc_width_pt, 0.95, (num_rows, num_cols), use_golden_ratio=False),\n",
    "    layout=\"constrained\",\n",
    ")\n",
    "for row_i, eval_names in enumerate(eval_namess):\n",
    "    for eval_i, eval_name in enumerate(eval_names):\n",
    "        ax = axes[row_i, eval_i]\n",
    "        for p_relevant_context in [0.0, 0.9, 1.0]:\n",
    "            data = 1 - (np.array(stats[\n",
    "                (stats[\"p_relevant_context\"] == f\"{p_relevant_context}\")\n",
    "                & (stats[\"input_noise_std\"] == f\"{input_noise_std}\")\n",
    "                & (stats[\"stats_key\"] == \"accuracy\")\n",
    "                & (stats[\"eval_name\"] == eval_name)\n",
    "                & (stats[\"dataset_size\"] == f\"{dataset_size}\")\n",
    "            ][\"stats\"].to_list()))\n",
    "\n",
    "            losses_mean = np.mean(data, axis=0)\n",
    "            losses_std = np.std(data, axis=0) / np.sqrt(5)\n",
    "\n",
    "            ax.plot(range(len(losses_mean)), losses_mean, label=map_label[p_relevant_context] if eval_i == 0 and row_i == 0 else \"\")\n",
    "            ax.fill_between(range(len(losses_mean)), losses_mean - losses_std, losses_mean + losses_std, alpha=0.3)\n",
    "\n",
    "        ax.set_title(map_eval[eval_name], fontsize=\"8\")\n",
    "        ax.axhline(9/10, label=\"Chance\" if eval_i == 0 and row_i == 0 else \"\", c=\"red\", linestyle=\"--\")\n",
    "        ax.set_ylim(-0.1, 1.1)\n",
    "\n",
    "        if eval_i == 0:\n",
    "            ax.set_ylabel(\"In-base Dist.\" if row_i == 0 else \"Out-of-base Dist.\", fontsize=\"8\")\n",
    "\n",
    "fig.supylabel(\"0-1 Error\", fontsize=\"8\")\n",
    "fig.supxlabel(\"Number of Gradient Steps ($\\\\times {}$)\".format(checkpoint_interval), fontsize=\"8\")\n",
    "fig.legend(\n",
    "    bbox_to_anchor=(0.0, 1.0, 1.0, 0.0),\n",
    "    loc=\"lower center\",\n",
    "    ncols=4,\n",
    "    borderaxespad=0.0,\n",
    "    frameon=True,\n",
    "    fontsize=\"8\", \n",
    ")\n",
    "# fig.tight_layout()\n",
    "plt.show()\n",
    "# plt.savefig(\"num_low_prob_classes-num_updates.pdf\", dpi=600, format=\"pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd5945a",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_label = {\n",
    "    0.0: \"IW Predictor\",\n",
    "    1.0: \"IC Predictor\",\n",
    "    0.9: \"Transformer\",\n",
    "}\n",
    "map_eval = {\n",
    "    \"eval-relevant_context-high_prob\": \"Rel. Context, High-freq.\",\n",
    "    \"eval-relevant_context-low_prob\": \"Rel. Context, Low-freq.\",\n",
    "    \"eval-irrelevant_context-high_prob\": \"Irrel. Context, High-freq.\",\n",
    "    \"eval-irrelevant_context-low_prob\": \"Irrel. Context, Low-freq.\",\n",
    "    \"eval-relevant_context-high_prob-flip_label\": \"Rel. Context, High-freq.\",\n",
    "    \"eval-relevant_context-low_prob-flip_label\": \"Rel. Context, Low-freq.\",\n",
    "    \"eval-irrelevant_context-high_prob-flip_label\": \"Irrel. Context, High-freq.\",\n",
    "    \"eval-irrelevant_context-low_prob-flip_label\": \"Irrel. Context, Low-freq.\",\n",
    "    \"eval-relevant_context-none\": \"Rel. Context, IBD\",\n",
    "    \"eval-relevant_context-none-flip_label\": \"Rel. Context, OOBD\",\n",
    "    \"eval-irrelevant_context-none\": \"Irrel. Context, IBD\",\n",
    "    \"eval-irrelevant_context-none-flip_label\": \"Irrel. Context, OOBD\",\n",
    "    \"pretraining\": \"Pretraining\",\n",
    "}\n",
    "\n",
    "eval_namess = [\n",
    "    [\n",
    "        \"eval-relevant_context-high_prob\",\n",
    "        \"eval-relevant_context-low_prob\",\n",
    "        \"eval-irrelevant_context-high_prob\",\n",
    "        \"eval-irrelevant_context-low_prob\",\n",
    "    ],\n",
    "    [\n",
    "        \"eval-relevant_context-high_prob-flip_label\",\n",
    "        \"eval-relevant_context-low_prob-flip_label\",\n",
    "        \"eval-irrelevant_context-high_prob-flip_label\",\n",
    "        \"eval-irrelevant_context-low_prob-flip_label\",\n",
    "    ],\n",
    "]\n",
    "\n",
    "\n",
    "input_noise_std = 0.4\n",
    "dataset_size = 16384\n",
    "checkpoint_interval = 500\n",
    "\n",
    "num_rows = 1\n",
    "num_cols = 1\n",
    "fig, axes = plt.subplots(\n",
    "    num_rows,\n",
    "    num_cols,\n",
    "    figsize=(5, 5),\n",
    "    layout=\"constrained\",\n",
    ")\n",
    "eval_name = \"pretraining\"\n",
    "ax = axes\n",
    "for p_relevant_context in [0.0]:\n",
    "    data = (np.array(stats[\n",
    "        (stats[\"p_relevant_context\"] == f\"{p_relevant_context}\")\n",
    "        & (stats[\"input_noise_std\"] == f\"{input_noise_std}\")\n",
    "        & (stats[\"stats_key\"] == \"loss\")\n",
    "        & (stats[\"eval_name\"] == eval_name)\n",
    "        & (stats[\"dataset_size\"] == f\"{dataset_size}\")\n",
    "    ][\"stats\"].to_list()))\n",
    "\n",
    "    losses_mean = np.mean(data, axis=0)\n",
    "    losses_std = np.std(data, axis=0) / np.sqrt(5)\n",
    "    print(p_relevant_context, np.min(losses_mean))\n",
    "\n",
    "    ax.plot(np.arange(len(losses_mean)) * 2, losses_mean, label=map_label[p_relevant_context])\n",
    "    ax.fill_between(np.arange(len(losses_mean)) * 2, losses_mean - losses_std, losses_mean + losses_std, alpha=0.3)\n",
    "\n",
    "ax.set_title(map_eval[eval_name], fontsize=\"8\")\n",
    "\n",
    "ax.set_ylim(0.0, 0.02)\n",
    "fig.supylabel(\"Cross-Entropy Loss\", fontsize=\"8\")\n",
    "fig.supxlabel(\"Number of Gradient Steps ($\\\\times {}$)\".format(500), fontsize=\"8\")\n",
    "fig.legend(\n",
    "    bbox_to_anchor=(0.0, 1.0, 1.0, 0.0),\n",
    "    loc=\"lower center\",\n",
    "    ncols=4,\n",
    "    borderaxespad=0.0,\n",
    "    frameon=True,\n",
    "    fontsize=\"8\", \n",
    ")\n",
    "# fig.tight_layout()\n",
    "plt.show()\n",
    "# plt.savefig(\"num_low_prob_classes-num_updates.pdf\", dpi=600, format=\"pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b498a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
