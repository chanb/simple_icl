{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "611aa6db-9102-41d8-a575-6da09450a2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import _pickle as pickle\n",
    "import copy\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as plticker\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import timeit\n",
    "\n",
    "from itertools import product\n",
    "from matplotlib.legend_handler import HandlerLine2D, HandlerTuple\n",
    "\n",
    "from utils import set_size, pgf_with_latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9bd8093",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"seaborn-v0_8-colorblind\")\n",
    "\n",
    "# Using the set_size function as defined earlier\n",
    "doc_width_pt = 452.9679"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1b7a68a-60d0-4758-b3f9-b5ef665b2722",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_keys = [\n",
    "    \"accuracy\",\n",
    "    \"p_iwl\",\n",
    "    \"context contains query class\",\n",
    "    \"loss\",\n",
    "    \"ic_pred\",\n",
    "    \"iw_pred\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dc71aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path = \"/Users/chanb/research/ualberta/icl/simple_icl\"\n",
    "results_dir = \"/Users/chanb/research/ualberta/icl/cc_results/paper_experiments/evaluation_results-heldout_features\"\n",
    "\n",
    "# repo_path = \"/home/bryanpu1/projects/icl/simple_icl\"\n",
    "# results_dir = \"/home/bryanpu1/projects/icl/scratch/evaluation_results\"\n",
    "# results_dir = \"/home/bryanpu1/projects/icl/cc_results/evaluation_results\"\n",
    "\n",
    "if repo_path.startswith(\"/Users\"):\n",
    "    plt.rcParams.update(pgf_with_latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e5fe6a4-d543-4e4e-9857-94bb492adc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_name = \"synthetic-transformer-noisy_inputs\"\n",
    "variant_name = \"omniglot-input_noise\"\n",
    "\n",
    "if variant_name.startswith(\"synthetic\"):\n",
    "    dataset_scale = np.log2\n",
    "    checkpoint_steps = 1000\n",
    "    dataset_unit = \"(in $\\\\log_{2}$)\"\n",
    "    num_seeds = 5\n",
    "    chance_error = 9/10\n",
    "else:\n",
    "    dataset_scale = np.log10\n",
    "    checkpoint_steps = 2500\n",
    "    dataset_unit = \"(in $\\\\log_{10}$)\"\n",
    "    num_seeds = 3\n",
    "    chance_error = 1622/1623\n",
    "\n",
    "stats_file = os.path.join(repo_path, \"plot_utils/plots/agg_stats\", \"{}-heldout.feather\".format(variant_name))\n",
    "stats = pd.read_feather(stats_file)\n",
    "\n",
    "os.makedirs(\n",
    "    os.path.join(repo_path, \"plot_utils/plots/acc-plots\", variant_name),\n",
    "    exist_ok=True\n",
    ")\n",
    "\n",
    "results_dir = os.path.join(results_dir, variant_name)\n",
    "plot_names = [\n",
    "    \"pretraining\",\n",
    "    \"context-default\",\n",
    "    \"iwl\",\n",
    "    \"iwl-flip_label\",\n",
    "    \"icl-last_context\",\n",
    "    \"icl-last_context-flip_label\",\n",
    "    \"icl-except_first_context\",\n",
    "    \"icl-except_first_context-flip_label\",\n",
    "]\n",
    "plot_titles = [\n",
    "    \"Pretraining\",\n",
    "    \"Context Default\"\n",
    "    \"In-weight Evaluation\",\n",
    "    \"In-weight Evaluation with Flipped Label\",\n",
    "    \"In-context Evaluation with Last Context\",\n",
    "    \"In-context Evaluation with Last Context + Flipped Label\",\n",
    "    \"In-context Evaluation with Contexts but First\",\n",
    "    \"In-context Evaluation with Contexts but First + Flipped Label\",\n",
    "]\n",
    "\n",
    "map_stats_key = {\n",
    "    \"p_iwl\": \"$\\\\alpha(x)$\",\n",
    "    \"accuracy\": \"Accuracy\",\n",
    "    \"context contains query class\": \"Context Contains Query Class\",\n",
    "    \"loss\": \"Loss\",\n",
    "    \"ic_pred\": \"In-context Accuracy\",\n",
    "    \"iw_pred\": \"In-weight Accuracy\"\n",
    "}\n",
    "\n",
    "map_variant = {\n",
    "    \"ground_truth_prob\": \"$P(g(x) = c)$\",\n",
    "    \"high_prob\": \"$P(high\\_freq.)$\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f9f88e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "variants = stats[\"variant\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b10f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats[\"eval_name\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040a7140",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d095e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "985cbd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sizes = sorted([int(dataset_size) for dataset_size in stats[\"dataset_size\"].unique()])\n",
    "input_noise_stds = sorted([float(input_noise_std) for input_noise_std in stats[\"input_noise_std\"].unique()])\n",
    "model_types = [\"transformer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dfab1b",
   "metadata": {},
   "source": [
    "# Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bb961e",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99892f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = {\n",
    "    \"iw\": \"--\",\n",
    "    \"ic\": \"-.\",\n",
    "    \"transformer\": \":\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14d2355e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_idxes = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea60e9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_label = {\n",
    "    \"iw\": \"IW Predictor\",\n",
    "    \"ic\": \"IC Predictor\",\n",
    "    \"transformer\": \"Transformer\",\n",
    "}\n",
    "\n",
    "p_relevant_context = 0.9\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(8, 3))\n",
    "for ax_i, model_type in enumerate(model_types):\n",
    "    ax = axes[ax_i]\n",
    "    for variant_i, input_noise_std in enumerate(input_noise_stds):\n",
    "        losses_mean = []\n",
    "        losses_std = []\n",
    "\n",
    "        for dataset_size in dataset_sizes:\n",
    "            data = (np.array(stats[\n",
    "                (stats[\"p_relevant_context\"] == f\"{p_relevant_context}\")\n",
    "                & (stats[\"input_noise_std\"] == f\"{input_noise_std}\")\n",
    "                & (stats[\"stats_key\"] == \"loss\")\n",
    "                & (stats[\"eval_name\"] == \"pretraining\")\n",
    "                & (stats[\"dataset_size\"] == f\"{dataset_size}\")\n",
    "                & (stats[\"model_type\"] == f\"{model_type}\")\n",
    "            ][\"stats\"].to_list()))\n",
    "            best_idx = data.shape[1] - np.argmin(data[:, ::-1], axis=-1) - 1\n",
    "\n",
    "            best_idxes[(\n",
    "                model_type, p_relevant_context, input_noise_std, dataset_size\n",
    "            )] = best_idx\n",
    "\n",
    "            sample = data[np.arange(num_seeds), best_idx]\n",
    "            curr_mean = np.mean(sample)\n",
    "            curr_std = np.std(sample)\n",
    "\n",
    "            losses_mean.append(curr_mean)\n",
    "            losses_std.append(curr_std)\n",
    "\n",
    "        losses_mean = np.array(losses_mean)\n",
    "        losses_std = np.array(losses_std) / np.sqrt(num_seeds)\n",
    "\n",
    "        ax.plot(dataset_scale(np.array(dataset_sizes)), losses_mean, label=input_noise_std if ax_i == 0 else \"\")\n",
    "        ax.fill_between(dataset_scale(np.array(dataset_sizes)), losses_mean - losses_std, losses_mean + losses_std, alpha=0.3)\n",
    "\n",
    "    ax.set_title(map_label[model_type])\n",
    "    # ax.set_ylim(0.0, 0.009)\n",
    "\n",
    "fig.supylabel(\"Cross-entropy Loss\")\n",
    "fig.supxlabel(\"Dataset Size {}\".format(dataset_unit))\n",
    "fig.legend(\n",
    "    bbox_to_anchor=(0.0, 1.0, 1.0, 0.0),\n",
    "    loc=\"lower center\",\n",
    "    ncols=4,\n",
    "    borderaxespad=0.0,\n",
    "    frameon=True,\n",
    "    fontsize=\"8\", \n",
    ")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec7c64d",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983ec8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(8, 3))\n",
    "for ax_i, model_type in enumerate(model_types):\n",
    "    ax = axes[ax_i]\n",
    "    for variant_i, input_noise_std in enumerate(input_noise_stds):\n",
    "        losses_mean = []\n",
    "        losses_std = []\n",
    "\n",
    "        for dataset_size in dataset_sizes:\n",
    "            data = 1 - (np.array(stats[\n",
    "                (stats[\"p_relevant_context\"] == f\"{p_relevant_context}\")\n",
    "                & (stats[\"input_noise_std\"] == f\"{input_noise_std}\")\n",
    "                & (stats[\"stats_key\"] == \"accuracy\")\n",
    "                & (stats[\"eval_name\"] == \"pretraining\")\n",
    "                & (stats[\"dataset_size\"] == f\"{dataset_size}\")\n",
    "                & (stats[\"model_type\"] == f\"{model_type}\")\n",
    "            ][\"stats\"].to_list()))\n",
    "            best_idx = best_idxes[(\n",
    "                model_type, p_relevant_context, input_noise_std, dataset_size\n",
    "            )]\n",
    "            sample = data[np.arange(num_seeds), best_idx]\n",
    "            curr_mean = np.mean(sample)\n",
    "            curr_std = np.std(sample)\n",
    "            \n",
    "            losses_mean.append(curr_mean)\n",
    "            losses_std.append(curr_std)\n",
    "\n",
    "        losses_mean = np.array(losses_mean)\n",
    "        losses_std = np.array(losses_std) / np.sqrt(num_seeds)\n",
    "\n",
    "        ax.plot(dataset_scale(np.array(dataset_sizes)), losses_mean, label=input_noise_std if ax_i == 0 else \"\")\n",
    "        ax.fill_between(dataset_scale(np.array(dataset_sizes)), losses_mean - losses_std, losses_mean + losses_std, alpha=0.3)\n",
    "\n",
    "    ax.set_title(map_label[model_type])\n",
    "    ax.set_ylim(0.0, 1.1)\n",
    "    ax.axhline(chance_error, label=\"Chance\" if ax_i == 0 else \"\", c=\"red\", linestyle=\"--\")\n",
    "\n",
    "fig.supylabel(\"0-1 Error\")\n",
    "fig.supxlabel(\"Dataset Size {}\".format(dataset_unit))\n",
    "fig.legend(\n",
    "    bbox_to_anchor=(0.0, 1.0, 1.0, 0.0),\n",
    "    loc=\"lower center\",\n",
    "    ncols=5,\n",
    "    borderaxespad=0.0,\n",
    "    frameon=True,\n",
    "    fontsize=\"8\", \n",
    ")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789213c5",
   "metadata": {},
   "source": [
    "## General Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60b7d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats[\"eval_name\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6989a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_eval = {\n",
    "    \"eval-relevant_context-high_prob\": \"Rel. Context, High-freq.\",\n",
    "    \"eval-relevant_context-low_prob\": \"Rel. Context, Low-freq.\",\n",
    "    \"eval-irrelevant_context-high_prob\": \"Irrel. Context, High-freq.\",\n",
    "    \"eval-irrelevant_context-low_prob\": \"Irrel. Context, Low-freq.\",\n",
    "    \"eval-relevant_context-high_prob-heldout_input-flipped_label\": \"Rel. Context, High-freq.\",\n",
    "    \"eval-relevant_context-low_prob-heldout_input-flipped_label\": \"Rel. Context, Low-freq.\",\n",
    "    \"eval-irrelevant_context-high_prob-heldout_input-flipped_label\": \"Irrel. Context, High-freq.\",\n",
    "    \"eval-irrelevant_context-low_prob-heldout_input-flipped_label\": \"Irrel. Context, Low-freq.\",\n",
    "}\n",
    "\n",
    "eval_namess = [\n",
    "    [\n",
    "        \"eval-relevant_context-high_prob-heldout_input-flipped_label\",\n",
    "        \"eval-relevant_context-low_prob-heldout_input-flipped_label\",\n",
    "        \"eval-irrelevant_context-high_prob-heldout_input-flipped_label\",\n",
    "        \"eval-irrelevant_context-low_prob-heldout_input-flipped_label\",\n",
    "    ],\n",
    "]\n",
    "\n",
    "for input_noise_std in input_noise_stds:\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(10, 5))\n",
    "    for row_i, eval_names in enumerate(eval_namess):\n",
    "        for eval_i, eval_name in enumerate(eval_names):\n",
    "            ax = axes[row_i, eval_i]\n",
    "            # ax = axes[eval_i]\n",
    "            for model_type in model_types:\n",
    "                losses_mean = []\n",
    "                losses_std = []\n",
    "\n",
    "                for dataset_size in dataset_sizes:\n",
    "                    data = (np.array(stats[\n",
    "                        (stats[\"p_relevant_context\"] == f\"{p_relevant_context}\")\n",
    "                        & (stats[\"input_noise_std\"] == f\"{input_noise_std}\")\n",
    "                        & (stats[\"stats_key\"] == \"loss\")\n",
    "                        & (stats[\"eval_name\"] == eval_name)\n",
    "                        & (stats[\"dataset_size\"] == f\"{dataset_size}\")\n",
    "                        & (stats[\"model_type\"] == f\"{model_type}\")\n",
    "                    ][\"stats\"].to_list()))\n",
    "                    best_idx = best_idxes[(\n",
    "                        model_type, p_relevant_context, input_noise_std, dataset_size\n",
    "                    )]\n",
    "                    sample = data[np.arange(num_seeds), best_idx]\n",
    "                    curr_mean = np.mean(sample)\n",
    "                    curr_std = np.std(sample)\n",
    "                    \n",
    "                    losses_mean.append(curr_mean)\n",
    "                    losses_std.append(curr_std)\n",
    "\n",
    "                losses_mean = np.array(losses_mean)\n",
    "                losses_std = np.array(losses_std) / np.sqrt(num_seeds)\n",
    "\n",
    "                ax.plot(dataset_scale(np.array(dataset_sizes)), losses_mean, label=map_label[model_type] if eval_i == 0 and row_i == 0 else \"\")\n",
    "                ax.fill_between(dataset_scale(np.array(dataset_sizes)), losses_mean - losses_std, losses_mean + losses_std, alpha=0.3)\n",
    "\n",
    "            ax.set_title(map_eval[eval_name])\n",
    "            # ax.set_ylim(-1.0, 1.5)\n",
    "            if eval_i == 0:\n",
    "                ax.set_ylabel(\"In-base Dist.\" if row_i == 0 else \"Out-of-base Dist.\")\n",
    "\n",
    "    fig.suptitle(\"Input Noise: {}\".format(input_noise_std))\n",
    "    fig.supylabel(\"Cross-entropy Loss\")\n",
    "    fig.supxlabel(\"Dataset Size {}\".format(dataset_unit))\n",
    "    fig.legend(\n",
    "        bbox_to_anchor=(0.0, 1.0, 1.0, 0.0),\n",
    "        loc=\"lower center\",\n",
    "        ncols=4,\n",
    "        borderaxespad=0.0,\n",
    "        frameon=True,\n",
    "        fontsize=\"8\", \n",
    "    )\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd7b28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_eval = {\n",
    "    \"eval-relevant_context-high_prob\": \"Rel. Context, High-freq.\",\n",
    "    \"eval-relevant_context-low_prob\": \"Rel. Context, Low-freq.\",\n",
    "    \"eval-irrelevant_context-high_prob\": \"Irrel. Context, High-freq.\",\n",
    "    \"eval-irrelevant_context-low_prob\": \"Irrel. Context, Low-freq.\",\n",
    "    \"eval-relevant_context-high_prob-heldout_input-flipped_label\": \"Rel. Context, High-freq.\",\n",
    "    \"eval-relevant_context-low_prob-heldout_input-flipped_label\": \"Rel. Context, Low-freq.\",\n",
    "    \"eval-irrelevant_context-high_prob-heldout_input-flipped_label\": \"Irrel. Context, High-freq.\",\n",
    "    \"eval-irrelevant_context-low_prob-heldout_input-flipped_label\": \"Irrel. Context, Low-freq.\",\n",
    "}\n",
    "\n",
    "eval_namess = [\n",
    "    [\n",
    "        \"eval-relevant_context-high_prob-heldout_input-flipped_label\",\n",
    "        \"eval-relevant_context-low_prob-heldout_input-flipped_label\",\n",
    "        \"eval-irrelevant_context-high_prob-heldout_input-flipped_label\",\n",
    "        \"eval-irrelevant_context-low_prob-heldout_input-flipped_label\",\n",
    "    ],\n",
    "]\n",
    "\n",
    "num_rows = 2\n",
    "num_cols = 4\n",
    "\n",
    "for input_noise_std in input_noise_stds:\n",
    "    fig, axes = plt.subplots(\n",
    "        num_rows,\n",
    "        num_cols,\n",
    "        figsize=set_size(doc_width_pt, 0.95, (num_rows, num_cols), use_golden_ratio=True),\n",
    "        layout=\"constrained\",\n",
    "    )\n",
    "    for row_i, eval_names in enumerate(eval_namess):\n",
    "        for eval_i, eval_name in enumerate(eval_names):\n",
    "            ax = axes[row_i, eval_i]\n",
    "            # ax = axes[eval_i]\n",
    "            ax.axhline(chance_error, label=\"Chance\" if eval_i == 0 and row_i == 0 else \"\", c=\"black\", linestyle=\"-\", alpha=0.5)\n",
    "            for model_type in model_types:\n",
    "                losses_mean = []\n",
    "                losses_std = []\n",
    "\n",
    "                for dataset_size in dataset_sizes:\n",
    "                    data = 1 - (np.array(stats[\n",
    "                        (stats[\"p_relevant_context\"] == f\"{p_relevant_context}\")\n",
    "                        & (stats[\"input_noise_std\"] == f\"{input_noise_std}\")\n",
    "                        & (stats[\"stats_key\"] == \"accuracy\")\n",
    "                        & (stats[\"eval_name\"] == eval_name)\n",
    "                        & (stats[\"dataset_size\"] == f\"{dataset_size}\")\n",
    "                        & (stats[\"model_type\"] == f\"{model_type}\")\n",
    "                    ][\"stats\"].to_list()))\n",
    "                    best_idx = best_idxes[(\n",
    "                        model_type, p_relevant_context, input_noise_std, dataset_size\n",
    "                    )]\n",
    "                    sample = data[np.arange(num_seeds), best_idx]\n",
    "                    curr_mean = np.mean(sample)\n",
    "                    curr_std = np.std(sample)\n",
    "                    \n",
    "                    losses_mean.append(curr_mean)\n",
    "                    losses_std.append(curr_std)\n",
    "\n",
    "                losses_mean = np.array(losses_mean)\n",
    "                losses_std = np.array(losses_std) / np.sqrt(num_seeds)\n",
    "\n",
    "                ax.plot(dataset_scale(np.array(dataset_sizes)), losses_mean, label=map_label[model_type] if eval_i == 0 and row_i == 0 else \"\", linestyle=ls[model_type])\n",
    "                ax.fill_between(dataset_scale(np.array(dataset_sizes)), losses_mean - losses_std, losses_mean + losses_std, alpha=0.3)\n",
    "\n",
    "            ax.set_ylim(-0.1, 1.1)\n",
    "            loc = plticker.MultipleLocator(base=4.0) # this locator puts ticks at regular intervals\n",
    "            ax.xaxis.set_major_locator(loc)\n",
    "\n",
    "            if eval_i == 0:\n",
    "                ax.set_ylabel(\"IBD\" if row_i == 0 else \"OOBD\", fontsize=\"8\",)\n",
    "\n",
    "            if eval_i > 0:\n",
    "                ax.set_yticks([])\n",
    "\n",
    "            if row_i == 0:\n",
    "                ax.set_title(map_eval[eval_name], fontsize=\"8\",)\n",
    "                ax.set_xticks([])\n",
    "\n",
    "    # fig.suptitle(\"$\\\\sigma$ = {}\".format(input_noise_std))\n",
    "    fig.supylabel(\"0-1 Error\", fontsize=\"8\",)\n",
    "    fig.supxlabel(\"Dataset Size {}\".format(dataset_unit), fontsize=\"8\",)\n",
    "    fig.legend(\n",
    "        bbox_to_anchor=(0.0, 1.0, 1.0, 0.0),\n",
    "        loc=\"lower center\",\n",
    "        ncols=4,\n",
    "        borderaxespad=0.0,\n",
    "        frameon=True,\n",
    "        fontsize=\"8\", \n",
    "    )\n",
    "    # fig.tight_layout()\n",
    "    if input_noise_std == 1.0:\n",
    "        plt.savefig(\"{}-noisy_inputs-input_noise_1.0-heldout.pdf\".format(variant_name), dpi=600, format=\"pdf\", bbox_inches=\"tight\")\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633974e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_eval = {\n",
    "    \"eval-relevant_context-high_prob-heldout_input-flipped_label\": \"Rel. Cont., $C_H$\",\n",
    "    \"eval-relevant_context-low_prob-heldout_input-flipped_label\": \"Rel. Cont., $C_L$\",\n",
    "    \"eval-irrelevant_context-high_prob-heldout_input-flipped_label\": \"Irrel. Cont., $C_H$\",\n",
    "    \"eval-irrelevant_context-low_prob-heldout_input-flipped_label\": \"Irrel. Cont., $C_L$\",\n",
    "}\n",
    "\n",
    "map_ls = {\n",
    "    0.0: \"--\",\n",
    "    0.1: \"-.\",\n",
    "    1.0: \":\",\n",
    "}\n",
    "\n",
    "dataset_scale = np.log10\n",
    "dataset_unit = \"(in $\\\\log_{10}$)\"\n",
    "num_seeds = 3\n",
    "chance_error = 1622/1623\n",
    "\n",
    "num_rows = 1\n",
    "num_cols = 4\n",
    "\n",
    "model_types = [\"iw\", \"ic\", \"transformer\"]\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    num_rows,\n",
    "    num_cols,\n",
    "    figsize=set_size(doc_width_pt, 0.95, (num_rows, num_cols), use_golden_ratio=False),\n",
    "    layout=\"constrained\",\n",
    ")\n",
    "\n",
    "\n",
    "for eval_i, eval_name in enumerate([\n",
    "        \"eval-relevant_context-high_prob-heldout_input-flipped_label\",\n",
    "        \"eval-relevant_context-low_prob-heldout_input-flipped_label\",\n",
    "        \"eval-irrelevant_context-high_prob-heldout_input-flipped_label\",\n",
    "        \"eval-irrelevant_context-low_prob-heldout_input-flipped_label\",\n",
    "]):    \n",
    "    ax = axes[eval_i]\n",
    "    ax.axhline(chance_error, label=\"Chance\" if eval_i == 0 else \"\", c=\"black\", linestyle=\"-\", alpha=0.5)\n",
    "    for input_noise_std in input_noise_stds:\n",
    "        losses_mean = []\n",
    "        losses_std = []\n",
    "        for dataset_size in dataset_sizes:\n",
    "            data = 1 - (np.array(stats[\n",
    "                (stats[\"p_relevant_context\"] == f\"0.9\")\n",
    "                & (stats[\"input_noise_std\"] == f\"{input_noise_std}\")\n",
    "                & (stats[\"stats_key\"] == \"accuracy\")\n",
    "                & (stats[\"eval_name\"] == eval_name)\n",
    "                & (stats[\"dataset_size\"] == f\"{dataset_size}\")\n",
    "                & (stats[\"model_type\"] == f\"transformer\")\n",
    "            ][\"stats\"].to_list()))\n",
    "            best_idx = best_idxes[(\n",
    "                model_type, p_relevant_context, input_noise_std, dataset_size\n",
    "            )]\n",
    "            sample = data[np.arange(num_seeds), best_idx]\n",
    "            curr_mean = np.mean(sample)\n",
    "            curr_std = np.std(sample)\n",
    "            \n",
    "            losses_mean.append(curr_mean)\n",
    "            losses_std.append(curr_std)\n",
    "\n",
    "        losses_mean = np.array(losses_mean)\n",
    "        losses_std = np.array(losses_std) / np.sqrt(num_seeds)\n",
    "\n",
    "        ax.plot(dataset_scale(np.array(dataset_sizes)), losses_mean, label=input_noise_std if eval_i == 0 else \"\", linestyle=map_ls[input_noise_std])\n",
    "        ax.fill_between(dataset_scale(np.array(dataset_sizes)), losses_mean - losses_std, losses_mean + losses_std, alpha=0.3)\n",
    "        ax.set_ylim(-0.1, 1.1)\n",
    "\n",
    "        ax.set_title(map_eval[eval_name], fontsize=\"8\",)\n",
    "        if eval_i > 0:\n",
    "            ax.set_yticks([])\n",
    "\n",
    "loc = plticker.MultipleLocator(base=1.0) # this locator puts ticks at regular intervals\n",
    "ax.xaxis.set_major_locator(loc)\n",
    "fig.supylabel(\"0-1 Error\", fontsize=\"8\",)\n",
    "fig.supxlabel(\"Dataset Size {}\".format(dataset_unit), fontsize=\"8\",)\n",
    "fig.legend(\n",
    "    bbox_to_anchor=(0.0, 1.0, 1.0, 0.0),\n",
    "    loc=\"lower center\",\n",
    "    ncols=4,\n",
    "    borderaxespad=0.0,\n",
    "    frameon=True,\n",
    "    fontsize=\"8\", \n",
    ")\n",
    "plt.savefig(\"omniglot-heldout_features.pdf\", dpi=600, format=\"pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaceadba",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats[\"eval_name\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff20de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_eval = {\n",
    "    \"eval-relevant_context-high_prob\": \"Rel. Context, High-freq.\",\n",
    "    \"eval-relevant_context-low_prob\": \"Rel. Context, Low-freq.\",\n",
    "    \"eval-irrelevant_context-high_prob\": \"Irrel. Context, High-freq.\",\n",
    "    \"eval-irrelevant_context-low_prob\": \"Irrel. Context, Low-freq.\",\n",
    "    \"eval-relevant_context-high_prob-heldout_input-flipped_label\": \"Rel. Context, High-freq.\",\n",
    "    \"eval-relevant_context-low_prob-heldout_input-flipped_label\": \"Rel. Context, Low-freq.\",\n",
    "    \"eval-irrelevant_context-high_prob-heldout_input-flipped_label\": \"Irrel. Context, High-freq.\",\n",
    "    \"eval-irrelevant_context-low_prob-heldout_input-flipped_label\": \"Irrel. Context, Low-freq.\",\n",
    "}\n",
    "\n",
    "eval_names = [\n",
    "    \"eval-relevant_context-high_prob-heldout_input-flipped_label\",\n",
    "    \"eval-relevant_context-low_prob-heldout_input-flipped_label\",\n",
    "    \"eval-irrelevant_context-high_prob-heldout_input-flipped_label\",\n",
    "    \"eval-irrelevant_context-low_prob-heldout_input-flipped_label\",\n",
    "]\n",
    "for input_noise_std in input_noise_stds:\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(10, 3))\n",
    "    for eval_i, eval_name in enumerate(eval_names):\n",
    "        ax = axes[eval_i]\n",
    "        for model_type in model_types:\n",
    "            losses_mean = []\n",
    "            losses_std = []\n",
    "\n",
    "            for dataset_size in dataset_sizes:\n",
    "                data = 1 - (np.array(stats[\n",
    "                    (stats[\"p_relevant_context\"] == f\"{p_relevant_context}\")\n",
    "                    & (stats[\"input_noise_std\"] == f\"{input_noise_std}\")\n",
    "                    & (stats[\"stats_key\"] == \"accuracy\")\n",
    "                    & (stats[\"eval_name\"] == eval_name)\n",
    "                    & (stats[\"dataset_size\"] == f\"{dataset_size}\")\n",
    "                    & (stats[\"model_type\"] == f\"{model_type}\")\n",
    "                ][\"stats\"].to_list()))\n",
    "                best_idx = best_idxes[(\n",
    "                    model_type, p_relevant_context, input_noise_std, dataset_size\n",
    "                )]\n",
    "                sample = data[np.arange(num_seeds), best_idx]\n",
    "                curr_mean = np.mean(sample)\n",
    "                curr_std = np.std(sample)\n",
    "                \n",
    "                losses_mean.append(curr_mean)\n",
    "                losses_std.append(curr_std)\n",
    "\n",
    "            losses_mean = np.array(losses_mean)\n",
    "            losses_std = np.array(losses_std) / np.sqrt(num_seeds)\n",
    "\n",
    "            ax.plot(dataset_scale(np.array(dataset_sizes)), losses_mean, label=map_label[model_type] if eval_i == 0 else \"\")\n",
    "            ax.fill_between(dataset_scale(np.array(dataset_sizes)), losses_mean - losses_std, losses_mean + losses_std, alpha=0.3)\n",
    "\n",
    "        ax.set_title(map_eval[eval_name])\n",
    "        ax.axhline(chance_error, label=\"Chance\" if eval_i == 0 else \"\", c=\"red\", linestyle=\"--\")\n",
    "        ax.set_ylim(-0.1, 1.1)\n",
    "\n",
    "    fig.suptitle(\"Input Noise: {}\".format(input_noise_std))\n",
    "    fig.supylabel(\"0-1 Error\")\n",
    "    fig.supxlabel(\"Dataset Size {}\".format(dataset_unit))\n",
    "    fig.legend(\n",
    "        bbox_to_anchor=(0.0, 1.0, 1.0, 0.0),\n",
    "        loc=\"lower center\",\n",
    "        ncols=4,\n",
    "        borderaxespad=0.0,\n",
    "        frameon=True,\n",
    "        fontsize=\"8\", \n",
    "    )\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd4c8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_eval = {\n",
    "    \"eval-relevant_context-high_prob\": \"Rel. Context, High-freq.\",\n",
    "    \"eval-relevant_context-low_prob\": \"Rel. Context, Low-freq.\",\n",
    "    \"eval-irrelevant_context-high_prob\": \"Irrel. Context, High-freq.\",\n",
    "    \"eval-irrelevant_context-low_prob\": \"Irrel. Context, Low-freq.\",\n",
    "    \"eval-relevant_context-high_prob-heldout_input-flipped_label\": \"Rel. Context, High-freq.\",\n",
    "    \"eval-relevant_context-low_prob-heldout_input-flipped_label\": \"Rel. Context, Low-freq.\",\n",
    "    \"eval-irrelevant_context-high_prob-heldout_input-flipped_label\": \"Irrel. Context, High-freq.\",\n",
    "    \"eval-irrelevant_context-low_prob-heldout_input-flipped_label\": \"Irrel. Context, Low-freq.\",\n",
    "    \"eval-default-none-heldout_input-flipped_label\": \"Heldout\"\n",
    "}\n",
    "\n",
    "eval_names = [\n",
    "    \"eval-default-none-heldout_input-flipped_label\",\n",
    "]\n",
    "\n",
    "num_rows = 2\n",
    "num_cols = len(input_noise_stds)\n",
    "fig, axes = plt.subplots(\n",
    "    num_rows,\n",
    "    num_cols,\n",
    "    figsize=set_size(doc_width_pt, 0.95, (num_rows, num_cols), use_golden_ratio=True),\n",
    "    layout=\"constrained\",\n",
    ")\n",
    "for col_i, input_noise_std in enumerate(input_noise_stds):\n",
    "    for eval_i, eval_name in enumerate(eval_names):\n",
    "        ax = axes[eval_i, col_i]\n",
    "        # ax = axes[eval_i]\n",
    "        for model_type in model_types:\n",
    "            losses_mean = []\n",
    "            losses_std = []\n",
    "\n",
    "            for dataset_size in dataset_sizes:\n",
    "                data = 1 - (np.array(stats[\n",
    "                    (stats[\"p_relevant_context\"] == f\"{p_relevant_context}\")\n",
    "                    & (stats[\"input_noise_std\"] == f\"{input_noise_std}\")\n",
    "                    & (stats[\"stats_key\"] == \"accuracy\")\n",
    "                    & (stats[\"eval_name\"] == eval_name)\n",
    "                    & (stats[\"dataset_size\"] == f\"{dataset_size}\")\n",
    "                    & (stats[\"model_type\"] == f\"{model_type}\")\n",
    "                ][\"stats\"].to_list()))\n",
    "                best_idx = best_idxes[(\n",
    "                    model_type, p_relevant_context, input_noise_std, dataset_size\n",
    "                )]\n",
    "                sample = data[np.arange(num_seeds), best_idx]\n",
    "                curr_mean = np.mean(sample)\n",
    "                curr_std = np.std(sample)\n",
    "                \n",
    "                losses_mean.append(curr_mean)\n",
    "                losses_std.append(curr_std)\n",
    "\n",
    "            losses_mean = np.array(losses_mean)\n",
    "            losses_std = np.array(losses_std) / np.sqrt(num_seeds)\n",
    "\n",
    "            ax.plot(dataset_scale(np.array(dataset_sizes)), losses_mean, label=map_label[model_type] if eval_i == 0 and col_i == 0 else \"\", alpha=1.0 if model_type == \"transformer\" else 0.5)\n",
    "            ax.fill_between(dataset_scale(np.array(dataset_sizes)), losses_mean - losses_std, losses_mean + losses_std, alpha=0.3)\n",
    "\n",
    "        ax.axhline(chance_error, label=\"Chance\" if eval_i == 0 and col_i == 0 else \"\", c=\"black\", linestyle=\"--\", alpha=0.5)\n",
    "        ax.set_ylim(-0.1, 1.1)\n",
    "        loc = plticker.MultipleLocator(base=1.0) # this locator puts ticks at regular intervals\n",
    "        ax.xaxis.set_major_locator(loc)\n",
    "\n",
    "        if col_i == 0:\n",
    "            ax.set_ylabel(map_eval[eval_name], fontsize=\"8\")\n",
    "\n",
    "        if col_i > 0:\n",
    "            ax.set_yticks([])\n",
    "\n",
    "        if eval_i == 0:\n",
    "            ax.set_xticks([])\n",
    "    \n",
    "    axes[0, col_i].set_title(f\"$\\\\sigma = {input_noise_std}$\", fontsize=\"8\")\n",
    "\n",
    "# fig.suptitle(\"Input Noise\")\n",
    "fig.supylabel(\"0-1 Error\", fontsize=\"8\")\n",
    "fig.supxlabel(\"Dataset Size {}\".format(dataset_unit), fontsize=\"8\")\n",
    "fig.legend(\n",
    "    bbox_to_anchor=(0.0, 1.0, 1.0, 0.0),\n",
    "    loc=\"lower center\",\n",
    "    ncols=4,\n",
    "    borderaxespad=0.0,\n",
    "    frameon=True,\n",
    "    fontsize=\"8\", \n",
    ")\n",
    "# fig.tight_layout()\n",
    "plt.savefig(\"{}-noisy_inputs-heldout.pdf\".format(variant_name), dpi=600, format=\"pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4680bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7ff4e7",
   "metadata": {},
   "source": [
    "# Slice based on Input noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfe4d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_eval = {\n",
    "    \"eval-default-high_prob\": \"Rel. Cont., $C_H$\",\n",
    "    \"eval-default-low_prob\": \"Rel. Cont., $C_L$\",\n",
    "    \"eval-relevant_context-high_prob\": \"Rel. Cont., $C_H$\",\n",
    "    \"eval-relevant_context-low_prob\": \"Rel. Cont., $C_L$\",\n",
    "    \"eval-irrelevant_context-high_prob\": \"Irrel. Cont., $C_H$\",\n",
    "    \"eval-irrelevant_context-low_prob\": \"Irrel. Cont., $C_L$\",\n",
    "    \"eval-relevant_context-high_prob-flip_label\": \"Rel. Cont., $C_H$\",\n",
    "    \"eval-relevant_context-low_prob-flip_label\": \"Rel. Cont., $C_L$\",\n",
    "    \"eval-irrelevant_context-high_prob-flip_label\": \"Irrel. Cont., $C_H$\",\n",
    "    \"eval-irrelevant_context-low_prob-flip_label\": \"Irrel. Cont., $C_L$\",\n",
    "    \"eval-relevant_context-none\": \"Rel. Context, IBD\",\n",
    "    \"eval-relevant_context-none-flip_label\": \"Rel. Context, OOBD\",\n",
    "    \"eval-irrelevant_context-none\": \"Irrel. Context, High-freq., IBD\",\n",
    "    \"eval-irrelevant_context-none-flip_label\": \"Irrel. Context, OOBD\",\n",
    "}\n",
    "\n",
    "eval_namess = [\n",
    "    [\n",
    "        \"eval-relevant_context-high_prob\",\n",
    "        \"eval-relevant_context-low_prob\",\n",
    "        \"eval-irrelevant_context-high_prob\",\n",
    "        \"eval-irrelevant_context-low_prob\",\n",
    "    ],\n",
    "    [\n",
    "        \"eval-relevant_context-high_prob-flip_label\",\n",
    "        \"eval-relevant_context-low_prob-flip_label\",\n",
    "        \"eval-irrelevant_context-high_prob-flip_label\",\n",
    "        \"eval-irrelevant_context-low_prob-flip_label\",\n",
    "    ],\n",
    "]\n",
    "\n",
    "num_rows = 2\n",
    "num_cols = 4\n",
    "fig, axes = plt.subplots(\n",
    "    num_rows,\n",
    "    num_cols,\n",
    "    # figsize=set_size(doc_width_pt, 0.95, (num_rows, num_cols), use_golden_ratio=True),\n",
    "    figsize=set_size(doc_width_pt, 0.95, (num_rows, num_cols), use_golden_ratio=False),\n",
    "    # figsize=(10, 5),\n",
    "    layout=\"constrained\",\n",
    ")\n",
    "\n",
    "dataset_size = dataset_sizes[0]\n",
    "print(dataset_size)\n",
    "input_noise_stds = [0.0, 0.2 ,0.4]\n",
    "for row_i, eval_names in enumerate(eval_namess):\n",
    "    for col_i, eval_name in enumerate(eval_names):\n",
    "\n",
    "        ax = axes[row_i, col_i]\n",
    "\n",
    "        for model_type in model_types:\n",
    "            losses_mean = []\n",
    "            losses_std = []\n",
    "            for input_noise_std in input_noise_stds:\n",
    "                data = 1 - (np.array(stats[\n",
    "                    (stats[\"p_relevant_context\"] == f\"{p_relevant_context}\")\n",
    "                    & (stats[\"input_noise_std\"] == f\"{input_noise_std}\")\n",
    "                    & (stats[\"stats_key\"] == \"accuracy\")\n",
    "                    & (stats[\"eval_name\"] == eval_name)\n",
    "                    & (stats[\"dataset_size\"] == f\"{dataset_size}\")\n",
    "                    & (stats[\"model_type\"] == f\"{model_type}\")\n",
    "                ][\"stats\"].to_list()))\n",
    "                best_idx = best_idxes[(\n",
    "                    model_type, p_relevant_context, input_noise_std, dataset_size\n",
    "                )]\n",
    "                sample = data[np.arange(num_seeds), best_idx]\n",
    "                curr_mean = np.mean(sample)\n",
    "                curr_std = np.std(sample)\n",
    "                \n",
    "                losses_mean.append(curr_mean)\n",
    "                losses_std.append(curr_std)\n",
    "\n",
    "            losses_mean = np.array(losses_mean)\n",
    "            losses_std = np.array(losses_std) / np.sqrt(num_seeds)\n",
    "\n",
    "            ax.plot(np.array(input_noise_stds) / 0.4, losses_mean, label=map_label[model_type] if row_i == 0 and col_i == 0 else \"\", marker=\"x\", alpha=1.0 if model_type == \"transformer\" else 0.5)\n",
    "            ax.fill_between(np.array(input_noise_stds) / 0.4, losses_mean - losses_std, losses_mean + losses_std, alpha=0.3)\n",
    "            ax.axhline(chance_error, label=\"Chance\" if eval_i == 0 and col_i == 0 else \"\", c=\"black\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "        ax.set_ylim(-0.1, 1.1)\n",
    "\n",
    "        if col_i == 0:\n",
    "            ax.set_ylabel(\"IBD\" if row_i == 0 else \"OOBD\", fontsize=\"8\")\n",
    "    \n",
    "        if col_i > 0:\n",
    "            ax.set_yticks([])\n",
    "\n",
    "        if row_i == 0:\n",
    "            ax.set_xticks([])\n",
    "            ax.set_title(map_eval[eval_name], fontsize=\"8\")\n",
    "\n",
    "# fig.suptitle(\"Input Noise\")\n",
    "fig.supylabel(\"0-1 Error\", fontsize=\"8\")\n",
    "fig.supxlabel(\"$\\\\sigma / 0.4$\", fontsize=\"8\")\n",
    "fig.legend(\n",
    "    bbox_to_anchor=(0.0, 1.0, 1.0, 0.0),\n",
    "    loc=\"lower center\",\n",
    "    ncols=4,\n",
    "    borderaxespad=0.0,\n",
    "    frameon=True,\n",
    "    fontsize=\"8\", \n",
    ")\n",
    "plt.savefig(\"{}-noisy_inputs-slice_by_sigma-heldout.pdf\".format(variant_name), dpi=600, format=\"pdf\", bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bad372",
   "metadata": {},
   "outputs": [],
   "source": [
    "axes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2645f063",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_names = [\n",
    "    \"eval-irrelevant_context-high_prob\",\n",
    "    \"eval-irrelevant_context-low_prob\",\n",
    "    \"eval-relevant_context-high_prob-flip_label\",\n",
    "    \"eval-relevant_context-low_prob-flip_label\",\n",
    "]\n",
    "\n",
    "num_rows = 2\n",
    "num_cols = 4\n",
    "\n",
    "if variant_name.startswith(\"synthetic\"):\n",
    "    map_ls = {\n",
    "        0.0: \"--\",\n",
    "        0.2: \"-.\",\n",
    "        0.4: \":\",\n",
    "    }\n",
    "    base = 4.0\n",
    "else:\n",
    "    map_ls = {\n",
    "        0.0: \"--\",\n",
    "        0.1: \"-.\",\n",
    "        1.0: \":\",\n",
    "    }\n",
    "    base = 1.0\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    num_rows,\n",
    "    num_cols,\n",
    "    figsize=set_size(doc_width_pt, 0.95, (num_rows, num_cols), use_golden_ratio=True),\n",
    "    layout=\"constrained\",\n",
    ")\n",
    "\n",
    "model_type = \"transformer\"\n",
    "for eval_i, eval_name in enumerate(eval_names):\n",
    "    ax = axes[int(eval_i >= 2), eval_i % 2]\n",
    "    # ax = axes[eval_i]\n",
    "    ax.axhline(chance_error, label=\"Chance\" if eval_i == 0 else \"\", c=\"black\", linestyle=\"-\", alpha=0.5)\n",
    "    for input_noise_std in input_noise_stds:\n",
    "        losses_mean = []\n",
    "        losses_std = []\n",
    "\n",
    "        for dataset_size in dataset_sizes:\n",
    "            data = 1 - (np.array(stats[\n",
    "                (stats[\"p_relevant_context\"] == f\"{p_relevant_context}\")\n",
    "                & (stats[\"input_noise_std\"] == f\"{input_noise_std}\")\n",
    "                & (stats[\"stats_key\"] == \"accuracy\")\n",
    "                & (stats[\"eval_name\"] == eval_name)\n",
    "                & (stats[\"dataset_size\"] == f\"{dataset_size}\")\n",
    "                & (stats[\"model_type\"] == f\"{model_type}\")\n",
    "            ][\"stats\"].to_list()))\n",
    "            best_idx = best_idxes[(\n",
    "                model_type, p_relevant_context, input_noise_std, dataset_size\n",
    "            )]\n",
    "            sample = data[np.arange(num_seeds), best_idx]\n",
    "            curr_mean = np.mean(sample)\n",
    "            curr_std = np.std(sample)\n",
    "            \n",
    "            losses_mean.append(curr_mean)\n",
    "            losses_std.append(curr_std)\n",
    "\n",
    "        losses_mean = np.array(losses_mean)\n",
    "        losses_std = np.array(losses_std) / np.sqrt(num_seeds)\n",
    "\n",
    "        ax.plot(dataset_scale(np.array(dataset_sizes)), losses_mean, label=input_noise_std if eval_i == 0 else \"\", linestyle=map_ls[input_noise_std])\n",
    "        ax.fill_between(dataset_scale(np.array(dataset_sizes)), losses_mean - losses_std, losses_mean + losses_std, alpha=0.3)\n",
    "\n",
    "    ax.set_ylim(-0.1, 1.1)\n",
    "    loc = plticker.MultipleLocator(base=base) # this locator puts ticks at regular intervals\n",
    "    ax.xaxis.set_major_locator(loc)\n",
    "\n",
    "    if eval_i % 2 == 0:\n",
    "        ax.set_ylabel(\"IWL\" if eval_i == 0 else \"ICL\", fontsize=\"8\",)\n",
    "\n",
    "    if eval_i % 2 != 0:\n",
    "        ax.set_yticks([])\n",
    "\n",
    "    if eval_i < 2:\n",
    "        ax.set_title(\"$C_H$\" if eval_i == 0 else \"$C_L$\", fontsize=\"8\",)\n",
    "        ax.set_xticks([])\n",
    "\n",
    "\n",
    "for eval_i, eval_name in enumerate(eval_names):\n",
    "    ax = axes[int(eval_i >= 2), 2 + eval_i % 2]\n",
    "    \n",
    "    model_type = \"iw\" if \"irrelevant_context\" in eval_name else \"ic\"\n",
    "    ax.axhline(chance_error, c=\"black\", linestyle=\"-\", alpha=0.5)\n",
    "    for input_noise_std in input_noise_stds:\n",
    "        losses_mean = []\n",
    "        losses_std = []\n",
    "\n",
    "        for dataset_size in dataset_sizes:\n",
    "            data = 1 - (np.array(stats[\n",
    "                (stats[\"p_relevant_context\"] == f\"{p_relevant_context}\")\n",
    "                & (stats[\"input_noise_std\"] == f\"{input_noise_std}\")\n",
    "                & (stats[\"stats_key\"] == \"accuracy\")\n",
    "                & (stats[\"eval_name\"] == eval_name)\n",
    "                & (stats[\"dataset_size\"] == f\"{dataset_size}\")\n",
    "                & (stats[\"model_type\"] == f\"{model_type}\")\n",
    "            ][\"stats\"].to_list()))\n",
    "            best_idx = best_idxes[(\n",
    "                model_type, p_relevant_context, input_noise_std, dataset_size\n",
    "            )]\n",
    "            sample = data[np.arange(num_seeds), best_idx]\n",
    "            curr_mean = np.mean(sample)\n",
    "            curr_std = np.std(sample)\n",
    "            \n",
    "            losses_mean.append(curr_mean)\n",
    "            losses_std.append(curr_std)\n",
    "\n",
    "        losses_mean = np.array(losses_mean)\n",
    "        losses_std = np.array(losses_std) / np.sqrt(num_seeds)\n",
    "\n",
    "        ax.plot(dataset_scale(np.array(dataset_sizes)), losses_mean, linestyle=map_ls[input_noise_std])\n",
    "        ax.fill_between(dataset_scale(np.array(dataset_sizes)), losses_mean - losses_std, losses_mean + losses_std, alpha=0.3)\n",
    "\n",
    "    ax.set_ylim(-0.1, 1.1)\n",
    "    loc = plticker.MultipleLocator(base=base) # this locator puts ticks at regular intervals\n",
    "    ax.xaxis.set_major_locator(loc)\n",
    "\n",
    "    if eval_i % 2 == 0:\n",
    "        ax.set_ylabel(\"IW Pred.\" if eval_i == 0 else \"IC Pred.\", fontsize=\"8\",)\n",
    "\n",
    "    if eval_i % 2 != 0:\n",
    "        ax.set_yticks([])\n",
    "\n",
    "    if eval_i < 2:\n",
    "        ax.set_title(\"$C_H$\" if eval_i == 0 else \"$C_L$\", fontsize=\"8\",)\n",
    "        ax.set_xticks([])\n",
    "\n",
    "\n",
    "\n",
    "# fig.suptitle(\"$\\\\sigma$ = {}\".format(input_noise_std))\n",
    "fig.supylabel(\"0-1 Error\", fontsize=\"8\",)\n",
    "fig.supxlabel(\"Dataset Size {}\".format(dataset_unit), fontsize=\"8\",)\n",
    "fig.legend(\n",
    "    bbox_to_anchor=(0.0, 1.0, 1.0, 0.0),\n",
    "    loc=\"lower center\",\n",
    "    ncols=4,\n",
    "    borderaxespad=0.0,\n",
    "    frameon=True,\n",
    "    fontsize=\"8\", \n",
    ")\n",
    "plt.savefig(\"{}-noisy_inputs-icl_analysis-heldout.pdf\".format(variant_name), dpi=600, format=\"pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefc7075",
   "metadata": {},
   "source": [
    "# Gradient Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "648b4a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = stats[stats[\"batches\"].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d725d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "caf63314",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sizes = [10000, 100000, 1000000]\n",
    "input_noise_std = 0.1\n",
    "seed = \"seed_2\"\n",
    "num_high_prob_classes = 20\n",
    "num_low_prob_classes = 1603"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "753b27b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_res = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3ad808ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_info_idx = 812\n",
    "# train_info = row[\"batches\"][train_info_idx]\n",
    "for dataset_size in dataset_sizes:\n",
    "    train_info = row[\n",
    "        (row[\"dataset_size\"] == \"{}\".format(dataset_size))\n",
    "        & (row[\"input_noise_std\"] == \"{}\".format(input_noise_std))\n",
    "        & (row[\"seed\"] == seed)\n",
    "    ]\n",
    "    targets = np.array([list(curr_it_targ) for curr_it_targ in train_info[\"batches\"].iloc[0][\"targets\"]])\n",
    "    counts = np.cumsum(np.sum(targets > num_high_prob_classes, axis=-1))\n",
    "\n",
    "    num_uniques = []\n",
    "    it_unique_mask = np.zeros(num_low_prob_classes + num_high_prob_classes)\n",
    "    for epoch_i in range(len(targets)):\n",
    "    # for epoch_i in range(10):\n",
    "        curr_unique, curr_counts = np.unique(targets[epoch_i], return_counts=True)\n",
    "        it_unique_mask[curr_unique] = it_unique_mask[curr_unique] + curr_counts\n",
    "        if (epoch_i + 1) % checkpoint_steps == 0:\n",
    "            num_uniques.append(copy.deepcopy(it_unique_mask))\n",
    "\n",
    "    unique_res[train_info[\"run_name\"].iloc[0]] = num_uniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b098a799",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_eval = {\n",
    "    \"eval-relevant_context-high_prob\": \"Rel. Cont., $C_H$\",\n",
    "    \"eval-relevant_context-low_prob\": \"Rel. Cont., $C_L$\",\n",
    "    \"eval-irrelevant_context-high_prob\": \"Irrel. Cont., $C_H$\",\n",
    "    \"eval-irrelevant_context-low_prob\": \"Irrel. Cont., $C_L$\",\n",
    "    \"eval-relevant_context-high_prob-flip_label\": \"Rel. Cont., $C_H$ OOBD\",\n",
    "    \"eval-relevant_context-low_prob-flip_label\": \"Rel. Cont., $C_L$ OOBD\",\n",
    "    \"eval-irrelevant_context-high_prob-flip_label\": \"Irrel. Cont., $C_H$ OOBD\",\n",
    "    \"eval-irrelevant_context-low_prob-flip_label\": \"Irrel. Cont., $C_L$ OOBD\",\n",
    "    \"pretraining\": \"Pretraining\"\n",
    "}\n",
    "\n",
    "eval_namess = [\n",
    "    [\n",
    "        \"eval-relevant_context-high_prob\",\n",
    "        \"eval-relevant_context-low_prob\",\n",
    "        \"eval-irrelevant_context-high_prob\",\n",
    "        \"eval-irrelevant_context-low_prob\",\n",
    "    ],\n",
    "    [\n",
    "        \"eval-relevant_context-high_prob-flip_label\",\n",
    "        \"eval-relevant_context-low_prob-flip_label\",\n",
    "        \"eval-irrelevant_context-high_prob-flip_label\",\n",
    "        \"eval-irrelevant_context-low_prob-flip_label\",\n",
    "    ],\n",
    "]\n",
    "\n",
    "c_map = {\n",
    "    \"eval-relevant_context-high_prob\": \"blue\",\n",
    "    \"eval-relevant_context-low_prob\": \"red\",\n",
    "    \"eval-irrelevant_context-high_prob\": \"blue\",\n",
    "    \"eval-irrelevant_context-low_prob\": \"red\",\n",
    "    \"eval-relevant_context-high_prob-flip_label\": \"blue\",\n",
    "    \"eval-relevant_context-low_prob-flip_label\": \"red\",\n",
    "    \"eval-irrelevant_context-high_prob-flip_label\": \"blue\",\n",
    "    \"eval-irrelevant_context-low_prob-flip_label\": \"red\",\n",
    "    \"pretraining\": \"gray\",\n",
    "}\n",
    "ls_map = {\n",
    "    \"eval-relevant_context-high_prob\": \":\",\n",
    "    \"eval-relevant_context-low_prob\": \":\",\n",
    "    \"eval-irrelevant_context-high_prob\": \":\",\n",
    "    \"eval-irrelevant_context-low_prob\": \":\",\n",
    "    \"eval-relevant_context-high_prob-flip_label\": \"-.\",\n",
    "    \"eval-relevant_context-low_prob-flip_label\": \"-.\",\n",
    "    \"eval-irrelevant_context-high_prob-flip_label\": \"-.\",\n",
    "    \"eval-irrelevant_context-low_prob-flip_label\": \"-.\",\n",
    "    \"pretraining\": \"-\",\n",
    "}\n",
    "\n",
    "\n",
    "num_rows = 1\n",
    "num_cols = 3\n",
    "fig, axes = plt.subplots(\n",
    "    num_rows,\n",
    "    num_cols,\n",
    "    figsize=set_size(doc_width_pt, 0.95, (num_rows, num_cols), use_golden_ratio=True),\n",
    "    # figsize=set_size(doc_width_pt, 0.65, (num_rows, num_cols), use_golden_ratio=True),\n",
    "    # figsize=(10, 5),\n",
    "    layout=\"constrained\",\n",
    ")\n",
    "\n",
    "plots = dict()\n",
    "\n",
    "for ax_i, key in enumerate(unique_res):\n",
    "    ax = axes[ax_i]\n",
    "    for eval_name in [\n",
    "        \"eval-relevant_context-high_prob\",\n",
    "        \"eval-relevant_context-low_prob\",\n",
    "        \"eval-relevant_context-high_prob-flip_label\",\n",
    "        \"eval-relevant_context-low_prob-flip_label\",\n",
    "        \"pretraining\",\n",
    "    ]:\n",
    "        res = 1 - np.array(stats[\n",
    "            (stats[\"run_name\"] == key)\n",
    "            & (stats[\"model_type\"] == \"transformer\")\n",
    "            & (stats[\"stats_key\"] == \"accuracy\")\n",
    "            & (stats[\"eval_name\"] == eval_name)\n",
    "        ][\"stats\"].tolist())[0, :-1]\n",
    "\n",
    "        plots[eval_name], = ax.plot(\n",
    "            np.arange(len(res)) * checkpoint_steps,\n",
    "            res,\n",
    "            label=map_eval[eval_name],\n",
    "            color=c_map[eval_name],\n",
    "            linestyle=ls_map[eval_name]\n",
    "        )\n",
    "    if ax_i > 0:\n",
    "        ax.set_yticks([])\n",
    "\n",
    "fig.supylabel(\"0-1 Error\", fontsize=\"8\")\n",
    "# fig.supxlabel(\"Gradient Step\", fontsize=\"8\")\n",
    "\n",
    "fake_bd, = ax.plot([], [], color = 'blue', linewidth = 1, label = 'my line', dashes=[9, 9], gapcolor='red')\n",
    "fake_ibd, = ax.plot([], [], color = 'gray', linewidth = 1, label = 'my line', linestyle=\":\")\n",
    "fake_oobd, = ax.plot([], [], color = 'gray', linewidth = 1, label = 'my line', linestyle=\"-.\")\n",
    "l = fig.legend(\n",
    "    [\n",
    "        (fake_bd,),\n",
    "        (fake_ibd,),\n",
    "        (fake_oobd,),\n",
    "        (plots[\"pretraining\"],)\n",
    "    ],\n",
    "    [\n",
    "        \"$C_H$/$C_L$\",\n",
    "        \"IBD\",\n",
    "        \"OOBD\",\n",
    "        \"Pretraining\"\n",
    "    ],\n",
    "    numpoints=1,\n",
    "    handler_map={tuple: HandlerTuple(ndivide=None)},\n",
    "    bbox_to_anchor=(0.0, 1.0, 1.0, 0.0),\n",
    "    loc=\"lower center\",\n",
    "    ncols=4,\n",
    "    borderaxespad=0.0,\n",
    "    frameon=True,\n",
    "    fontsize=\"8\", \n",
    ")\n",
    "\n",
    "plt.savefig(\"{}-noisy_inputs-grad_steps-{}-heldout.pdf\".format(variant_name, seed), dpi=600, format=\"pdf\", bbox_inches=\"tight\")\n",
    "# plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6b6484",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_rows = 1\n",
    "num_cols = 3\n",
    "fig, axes = plt.subplots(\n",
    "    num_rows,\n",
    "    num_cols,\n",
    "    # figsize=set_size(doc_width_pt, 0.95, (num_rows, num_cols), use_golden_ratio=True),\n",
    "    figsize=set_size(doc_width_pt, 0.95, (num_rows, num_cols), use_golden_ratio=True),\n",
    "    # figsize=(10, 5),\n",
    "    layout=\"constrained\",\n",
    ")\n",
    "\n",
    "for ax_i, key in enumerate(unique_res):\n",
    "    ax = axes[ax_i]\n",
    "    unique_i = unique_res[key]\n",
    "    means = np.array([np.mean(curr_unique[num_high_prob_classes:]) for checkpoint_i, curr_unique in enumerate(unique_i)])\n",
    "    medians = np.array([np.median(curr_unique[num_high_prob_classes:]) for checkpoint_i, curr_unique in enumerate(unique_i)])\n",
    "    maxes = np.array([np.max(curr_unique[num_high_prob_classes:]) for checkpoint_i, curr_unique in enumerate(unique_i)])\n",
    "    mins = np.array([np.min(curr_unique[num_high_prob_classes:]) for checkpoint_i, curr_unique in enumerate(unique_i)])\n",
    "\n",
    "\n",
    "    ax.plot(\n",
    "        np.arange(len(res)) * checkpoint_steps,\n",
    "        means,\n",
    "        linestyle=\"--\",\n",
    "        color=\"blue\",\n",
    "    )\n",
    "    ax.plot(\n",
    "        np.arange(len(res)) * checkpoint_steps,\n",
    "        medians,\n",
    "        color=\"red\",\n",
    "    )\n",
    "\n",
    "    ax.fill_between(\n",
    "        np.arange(len(res)) * checkpoint_steps,\n",
    "        maxes,\n",
    "        mins,\n",
    "        color=\"blue\",\n",
    "        alpha=0.3\n",
    "    )\n",
    "\n",
    "    ax.set_ylim(0, 500)\n",
    "\n",
    "    if ax_i > 0:\n",
    "        ax.set_yticks([])\n",
    "fig.supylabel(\"Total Occurrence\", fontsize=\"8\")\n",
    "fig.supxlabel(\"Gradient Step\", fontsize=\"8\")\n",
    "plt.savefig(\"{}-noisy_inputs-low_freq_occ-{}-heldout.pdf\".format(variant_name, seed), dpi=600, format=\"pdf\", bbox_inches=\"tight\")\n",
    "# plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797c8ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ax_i, key in enumerate(unique_res):\n",
    "    unique_i = unique_res[key]\n",
    "    means = np.array([np.mean(curr_unique[num_high_prob_classes:]) for checkpoint_i, curr_unique in enumerate(unique_i)])\n",
    "    print(means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff822170",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
