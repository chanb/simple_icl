{
    "logging_config": {
        "save_path": "<TO_FILL>",
        "experiment_name": "<TO_FILL>",
        "log_interval": 50,
        "checkpoint_interval": 1000
    },
    "model_config": {
        "architecture": "InContextSupervisedTransformer",
        "model_kwargs": {
            "output_dim": 500,
            "num_contexts": 4,
            "num_blocks": 4,
            "num_heads": 4,
            "embed_dim": 64,
            "widening_factor": 4,
            "query_pred_only": true
        }
    },
    "optimizer_config": {
        "optimizer": "adam",
        "lr": {
            "scheduler": "constant_schedule",
            "scheduler_kwargs": {
                "value": 3e-4
            }
        },
        "opt_kwargs": {},
        "max_grad_norm": false,
        "mask_names": ["input_tokenizer"]
    },
    "dataset_name": "synthetic",
    "dataset_kwargs": {
        "dataset_size": "<TO_FILL>",
        "num_high_prob_classes": 5,
        "num_low_prob_classes": 495,
        "p_high": 0.9,
        "p_relevant_context": "<TO_FILL>",
        "num_dims": 64,
        "train": true,
        "num_contexts": 4,
        "input_noise_std": "<TO_FILL>",
        "label_noise": 0.0
    },
    "shuffle_buffer_size": 100,
    "num_workers": -1,
    "batch_size": 32,
    "num_epochs": 100000,
    "num_updates_per_epoch": 1,
    "seeds": {
        "learner_seed": 46,
        "data_seed": 46
    }
}