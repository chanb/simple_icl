{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OiBSu3YkEcoX"
   },
   "source": [
    "Copyright 2024 DeepMind Technologies Limited.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "   http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y5OeTiryEcoX"
   },
   "source": [
    "# Fine-tuning the 2B Gemma model with flax\n",
    "\n",
    "In this tutorial you will learn how to fine-tune the 2B Gemma model for a simple translation task. To run this colab, you will need to use a TPU v4 runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5m81VQOqEcoX"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "XpSw-_4EEcoY",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/google-deepmind/gemma.git\n",
      "  Cloning https://github.com/google-deepmind/gemma.git to /tmp/pip-req-build-hxirukrg\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/google-deepmind/gemma.git /tmp/pip-req-build-hxirukrg\n",
      "  Resolved https://github.com/google-deepmind/gemma.git to commit 2ea41628173cd88de9ab6963e628889faec86ff5\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: absl-py<3.0.0,>=2.1.0 in /home/bryanpu1/.conda/envs/simple_icl/lib/python3.10/site-packages (from gemma==1.0.0) (2.1.0)\n",
      "Requirement already satisfied: flax>=0.8 in /home/bryanpu1/.conda/envs/simple_icl/lib/python3.10/site-packages (from gemma==1.0.0) (0.9.0)\n",
      "Requirement already satisfied: sentencepiece<0.2.0,>=0.1.99 in /home/bryanpu1/.conda/envs/simple_icl/lib/python3.10/site-packages (from gemma==1.0.0) (0.1.99)\n",
      "Requirement already satisfied: jax>=0.4.27 in /home/bryanpu1/.conda/envs/simple_icl/lib/python3.10/site-packages (from flax>=0.8->gemma==1.0.0) (0.4.33)\n",
      "Requirement already satisfied: msgpack in /home/bryanpu1/.conda/envs/simple_icl/lib/python3.10/site-packages (from flax>=0.8->gemma==1.0.0) (1.1.0)\n",
      "Requirement already satisfied: optax in /home/bryanpu1/.conda/envs/simple_icl/lib/python3.10/site-packages (from flax>=0.8->gemma==1.0.0) (0.2.3)\n",
      "Requirement already satisfied: orbax-checkpoint in /home/bryanpu1/.conda/envs/simple_icl/lib/python3.10/site-packages (from flax>=0.8->gemma==1.0.0) (0.6.4)\n",
      "Requirement already satisfied: tensorstore in /home/bryanpu1/.conda/envs/simple_icl/lib/python3.10/site-packages (from flax>=0.8->gemma==1.0.0) (0.1.66)\n",
      "Requirement already satisfied: rich>=11.1 in /home/bryanpu1/.conda/envs/simple_icl/lib/python3.10/site-packages (from flax>=0.8->gemma==1.0.0) (13.8.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in /home/bryanpu1/.conda/envs/simple_icl/lib/python3.10/site-packages (from flax>=0.8->gemma==1.0.0) (4.12.2)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /home/bryanpu1/.conda/envs/simple_icl/lib/python3.10/site-packages (from flax>=0.8->gemma==1.0.0) (6.0.2)\n",
      "Requirement already satisfied: jaxlib<=0.4.33,>=0.4.33 in /home/bryanpu1/.conda/envs/simple_icl/lib/python3.10/site-packages (from jax>=0.4.27->flax>=0.8->gemma==1.0.0) (0.4.33)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in /home/bryanpu1/.conda/envs/simple_icl/lib/python3.10/site-packages (from jax>=0.4.27->flax>=0.8->gemma==1.0.0) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.24 in /home/bryanpu1/.conda/envs/simple_icl/lib/python3.10/site-packages (from jax>=0.4.27->flax>=0.8->gemma==1.0.0) (1.26.4)\n",
      "Requirement already satisfied: opt-einsum in /home/bryanpu1/.conda/envs/simple_icl/lib/python3.10/site-packages (from jax>=0.4.27->flax>=0.8->gemma==1.0.0) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.10 in /home/bryanpu1/.conda/envs/simple_icl/lib/python3.10/site-packages (from jax>=0.4.27->flax>=0.8->gemma==1.0.0) (1.14.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/bryanpu1/.conda/envs/simple_icl/lib/python3.10/site-packages (from rich>=11.1->flax>=0.8->gemma==1.0.0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/bryanpu1/.conda/envs/simple_icl/lib/python3.10/site-packages (from rich>=11.1->flax>=0.8->gemma==1.0.0) (2.18.0)\n",
      "Requirement already satisfied: chex>=0.1.86 in /home/bryanpu1/.conda/envs/simple_icl/lib/python3.10/site-packages (from optax->flax>=0.8->gemma==1.0.0) (0.1.87)\n",
      "Requirement already satisfied: etils[epy] in /home/bryanpu1/.conda/envs/simple_icl/lib/python3.10/site-packages (from optax->flax>=0.8->gemma==1.0.0) (1.9.4)\n",
      "Requirement already satisfied: nest_asyncio in /home/bryanpu1/.conda/envs/simple_icl/lib/python3.10/site-packages (from orbax-checkpoint->flax>=0.8->gemma==1.0.0) (1.6.0)\n",
      "Requirement already satisfied: protobuf in /home/bryanpu1/.conda/envs/simple_icl/lib/python3.10/site-packages (from orbax-checkpoint->flax>=0.8->gemma==1.0.0) (3.20.3)\n",
      "Requirement already satisfied: humanize in /home/bryanpu1/.conda/envs/simple_icl/lib/python3.10/site-packages (from orbax-checkpoint->flax>=0.8->gemma==1.0.0) (4.10.0)\n",
      "Requirement already satisfied: toolz>=0.9.0 in /home/bryanpu1/.conda/envs/simple_icl/lib/python3.10/site-packages (from chex>=0.1.86->optax->flax>=0.8->gemma==1.0.0) (0.12.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/bryanpu1/.conda/envs/simple_icl/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax>=0.8->gemma==1.0.0) (0.1.2)\n",
      "Requirement already satisfied: fsspec in /home/bryanpu1/.conda/envs/simple_icl/lib/python3.10/site-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.8->gemma==1.0.0) (2024.9.0)\n",
      "Requirement already satisfied: importlib_resources in /home/bryanpu1/.conda/envs/simple_icl/lib/python3.10/site-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.8->gemma==1.0.0) (6.4.5)\n",
      "Requirement already satisfied: zipp in /home/bryanpu1/.conda/envs/simple_icl/lib/python3.10/site-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.8->gemma==1.0.0) (3.20.2)\n",
      "Requirement already satisfied: kaggle in /home/bryanpu1/.local/lib/python3.10/site-packages (1.6.17)\n",
      "Requirement already satisfied: six>=1.10 in /home/bryanpu1/.conda/envs/simple_icl/lib/python3.10/site-packages (from kaggle) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2023.7.22 in /home/bryanpu1/.conda/envs/simple_icl/lib/python3.10/site-packages (from kaggle) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil in /home/bryanpu1/.conda/envs/simple_icl/lib/python3.10/site-packages (from kaggle) (2.9.0.post0)\n",
      "Requirement already satisfied: requests in /home/bryanpu1/.conda/envs/simple_icl/lib/python3.10/site-packages (from kaggle) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /home/bryanpu1/.conda/envs/simple_icl/lib/python3.10/site-packages (from kaggle) (4.66.5)\n",
      "Requirement already satisfied: python-slugify in /home/bryanpu1/.local/lib/python3.10/site-packages (from kaggle) (8.0.4)\n",
      "Requirement already satisfied: urllib3 in /home/bryanpu1/.conda/envs/simple_icl/lib/python3.10/site-packages (from kaggle) (2.2.3)\n",
      "Requirement already satisfied: bleach in /home/bryanpu1/.conda/envs/simple_icl/lib/python3.10/site-packages (from kaggle) (6.1.0)\n",
      "Requirement already satisfied: webencodings in /home/bryanpu1/.conda/envs/simple_icl/lib/python3.10/site-packages (from bleach->kaggle) (0.5.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /home/bryanpu1/.local/lib/python3.10/site-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/bryanpu1/.conda/envs/simple_icl/lib/python3.10/site-packages (from requests->kaggle) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/bryanpu1/.conda/envs/simple_icl/lib/python3.10/site-packages (from requests->kaggle) (3.10)\n",
      "Requirement already satisfied: kagglehub in /home/bryanpu1/.local/lib/python3.10/site-packages (0.3.0)\n",
      "Requirement already satisfied: packaging in /home/bryanpu1/.conda/envs/simple_icl/lib/python3.10/site-packages (from kagglehub) (24.1)\n",
      "Requirement already satisfied: requests in /home/bryanpu1/.conda/envs/simple_icl/lib/python3.10/site-packages (from kagglehub) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /home/bryanpu1/.conda/envs/simple_icl/lib/python3.10/site-packages (from kagglehub) (4.66.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/bryanpu1/.conda/envs/simple_icl/lib/python3.10/site-packages (from requests->kagglehub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/bryanpu1/.conda/envs/simple_icl/lib/python3.10/site-packages (from requests->kagglehub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/bryanpu1/.conda/envs/simple_icl/lib/python3.10/site-packages (from requests->kagglehub) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/bryanpu1/.conda/envs/simple_icl/lib/python3.10/site-packages (from requests->kagglehub) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "# @title Installation\n",
    "! pip install git+https://github.com/google-deepmind/gemma.git\n",
    "! pip install --user kaggle\n",
    "! pip install --user kagglehub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iLafhtv3Rg5F"
   },
   "source": [
    "## Downloading the checkpoint\n",
    "\n",
    "\"To use Gemma's checkpoints, you'll need a Kaggle account and API key. Here's how to get them:\n",
    "\n",
    "1. Visit https://www.kaggle.com/ and create an account.\n",
    "2. Go to your account settings, then the 'API' section.\n",
    "3. Click 'Create new token' to download your key.\n",
    "\n",
    "Then run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8q5seOhcUBhx"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43e9ea5b91874520a9aea8aa9ee49e5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://www.kaggle.com/static/images/site-logo.png\\nalt=\\'Kaggleâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import kagglehub\n",
    "kagglehub.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jCZSmEVDVv6O"
   },
   "source": [
    "If everything went well, you should see:\n",
    "```\n",
    "Kaggle credentials set.\n",
    "Kaggle credentials successfully validated.\n",
    "```\n",
    "\n",
    "Now select and download the checkpoint you want to try. On a single host, only the 2b model can fit in memory for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9PEefz8wEcoY",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "VARIANT = '2b' # @param ['2b', '2b-it'] {type:\"string\"}\n",
    "weights_dir = kagglehub.model_download(f'google/gemma/Flax/{VARIANT}')\n",
    "ckpt_path = os.path.join(weights_dir, VARIANT)\n",
    "vocab_path = os.path.join(weights_dir, 'tokenizer.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "yWaP_LPoEcoY"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-30 16:23:51.119108: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-30 16:23:51.137025: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-30 16:23:51.142515: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-30 16:23:51.932965: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# @title Python imports\n",
    "\n",
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.0\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\"\n",
    "# os.environ[\"XLA_FLAGS\"] = \"--xla_gpu_enable_command_buffer=\"\n",
    "\n",
    "import enum\n",
    "import re\n",
    "import string\n",
    "\n",
    "# We import JAX and some related packages.\n",
    "import chex\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# We will use tensorflow to handle the dataset\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Finally, we import Gemma.\n",
    "from gemma import params as params_lib\n",
    "from gemma import sampler as sampler_lib\n",
    "from gemma import transformer as transformer_lib\n",
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejQhgtjbEcoY"
   },
   "source": [
    "## Step 1: prepare the dataset\n",
    "Each sample in the dataset contains two entries:\n",
    "- 'src': The original English sentence.\n",
    "- 'dst': The corresponding French translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NYC42hJgEcoY"
   },
   "source": [
    "### Tokenizer\n",
    "\n",
    "Let's start by loading our vocabulary base tokenizer, which we'll construct using the [SentencePiece](https://github.com/google/sentencepiece) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "form",
    "id": "TpyG5YW1EcoY"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = spm.SentencePieceProcessor()\n",
    "vocab.Load(vocab_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ab2MSf-qEcoY"
   },
   "source": [
    "Let's customize `SentencePieceProcessor` for our English-to-French translation task. Since we're fine-tuning the English-only Gemma 2B model, we need a few adjustments:\n",
    "\n",
    "- **Input Prefix**: Adding a common prefix to each input signals the translation task. For example we could go with a prompt like `Translate this into French: [INPUT_SENTENCE]`.\n",
    "\n",
    "- **Translation Start suffix**: We add a suffix at the end of each prompt tells the model exactly when to begin the translation process. A new line should do the job.\n",
    "\n",
    "- **LM Tokens**: Gemma models expect a *beginning of sequence* token at the beginning of each sequence. Similarly, we need to add an *end of sequence* token at the end of each training example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "form",
    "id": "L9cjK0uxEcoY"
   },
   "outputs": [],
   "source": [
    "class GemmaTokenizer:\n",
    "  \"\"\"Custom wrapper around a SentencePieceProcessor for tensorflow.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               spm_processor: spm.SentencePieceProcessor):\n",
    "    self._spm_processor = spm_processor\n",
    "\n",
    "  @property\n",
    "  def pad_id(self) -> int:\n",
    "    \"\"\"Fast access to the pad id.\"\"\"\n",
    "    return self._spm_processor.pad_id()\n",
    "\n",
    "  def tokenize(self,\n",
    "               example: str | bytes,\n",
    "               prefix: str = '',\n",
    "               suffix: str = '',\n",
    "               add_bos: bool = True,\n",
    "               add_eos: bool = True) -> jax.Array:\n",
    "    \"\"\"\n",
    "    Tokenization function.\n",
    "\n",
    "    Args:\n",
    "      example: input string to tokenize.\n",
    "      prefix:  prefix to add to the input string.\n",
    "      suffix:  suffix to add to the input string.\n",
    "      add_bos: if True, add a beginning of sequence token at the start of the\n",
    "               tokenized sequence.\n",
    "      add_eos: if True, add an end of sequence token at the end of the tokenized\n",
    "               sequence.\n",
    "    Returns:\n",
    "      Tokens corresponding to the input string.\n",
    "    \"\"\"\n",
    "    int_list = []\n",
    "    if add_bos:\n",
    "      int_list.append(self._spm_processor.bos_id())\n",
    "    int_list.extend(self._spm_processor.EncodeAsIds(prefix + example + suffix))\n",
    "    if add_eos:\n",
    "      int_list.append(self._spm_processor.eos_id())\n",
    "\n",
    "    return jnp.array(int_list, dtype=jnp.int32)\n",
    "\n",
    "  def tokenize_tf_op(self,\n",
    "                     str_tensor: tf.Tensor,\n",
    "                     prefix: str = '',\n",
    "                     suffix: str = '',\n",
    "                     add_bos: bool = True,\n",
    "                     add_eos: bool = True) -> tf.Tensor:\n",
    "    \"\"\"Tensforflow operator for the tokenize function.\"\"\"\n",
    "    encoded = tf.numpy_function(\n",
    "        self.tokenize,\n",
    "        [str_tensor, prefix, suffix, add_bos, add_eos],\n",
    "        tf.int32)\n",
    "    encoded.set_shape([None])\n",
    "    return encoded\n",
    "\n",
    "  def to_string(self, tokens: jax.Array) -> str:\n",
    "    \"\"\"Convert an array of tokens to a string.\"\"\"\n",
    "    return self._spm_processor.DecodeIds(tokens.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6xuCVkurEcoY"
   },
   "source": [
    "Now let's try our custom tokenizer on the MTNT dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-x0aTugEcoY"
   },
   "source": [
    "### Data loader\n",
    "\n",
    "We can now wrap everything a build our data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@chex.dataclass(frozen=True)\n",
    "class TrainingInput:\n",
    "  # Input tokens given to the model\n",
    "  input_tokens: jax.Array\n",
    "\n",
    "  # A mask that determines which tokens contribute to the target loss\n",
    "  # calculation.\n",
    "  target_mask: jax.Array\n",
    "\n",
    "class DatasetSplit(enum.Enum):\n",
    "  TRAIN = 'train'\n",
    "  VALIDATION = 'valid'\n",
    "\n",
    "src_map = [\n",
    "    \"Cxx1h\",\n",
    "    \"Vzbiik\",\n",
    "    \"Mmojkr\",\n",
    "    \"Trrrrqe\",\n",
    "    \"Benjamin\",\n",
    "    \"Liz\",\n",
    "    \"Kaitlyn\",\n",
    "    \"Wiesa\",\n",
    "]\n",
    "\n",
    "dst_map = [\n",
    "    \"Lkkl\",\n",
    "    \"Plooqujhd\",\n",
    "    \"Nwops\",\n",
    "    \"Qtbnaaa\",\n",
    "    \"Buenos\",\n",
    "    \"London\",\n",
    "    \"Kingston\",\n",
    "    \"Warsaw\",\n",
    "]\n",
    "\n",
    "def seq_generator():\n",
    "    rng = np.random.RandomState(42)\n",
    "    while True:\n",
    "        idx = rng.randint(len(src_map))\n",
    "        yield {\n",
    "            \"src\": idx,\n",
    "            \"dst\": idx,\n",
    "        }\n",
    "\n",
    "\n",
    "class FTDatasetBuilder:\n",
    "  \"\"\"Data loader for the FT dataset.\"\"\"\n",
    "\n",
    "  N_ITEMS = {DatasetSplit.TRAIN: 10,\n",
    "             DatasetSplit.VALIDATION: 10}\n",
    "\n",
    "  BUFFER_SIZE_SHUFFLE = 10\n",
    "  Q_PREFIX = 'Where does '\n",
    "  Q_SUFFIX = ' live? Only give the name of the city.'\n",
    "\n",
    "  def __init__(self,\n",
    "               tokenizer : GemmaTokenizer,\n",
    "               max_seq_len: int):\n",
    "    \"\"\"Constructor.\n",
    "\n",
    "    Args:\n",
    "      tokenizer: Gemma tokenizer to use.\n",
    "      max_seq_len: size of each sequence in a given batch.\n",
    "    \"\"\"\n",
    "    self._tokenizer = tokenizer\n",
    "    self._base_data = {\n",
    "        DatasetSplit.TRAIN: tf.data.Dataset.from_generator(\n",
    "            seq_generator,\n",
    "            output_signature={\n",
    "                \"src\": tf.TensorSpec(\n",
    "                    shape=(),\n",
    "                    dtype=tf.dtypes.int32,\n",
    "                ),\n",
    "                \"dst\": tf.TensorSpec(\n",
    "                    shape=(),\n",
    "                    dtype=tf.dtypes.int32,\n",
    "                ),\n",
    "            },\n",
    "        ),\n",
    "        DatasetSplit.VALIDATION: tf.data.Dataset.from_generator(\n",
    "            seq_generator,\n",
    "            output_signature={\n",
    "                \"src\": tf.TensorSpec(\n",
    "                    shape=(),\n",
    "                    dtype=tf.dtypes.int32,\n",
    "                ),\n",
    "                \"dst\": tf.TensorSpec(\n",
    "                    shape=(),\n",
    "                    dtype=tf.dtypes.int32,\n",
    "                ),\n",
    "            },\n",
    "        ),\n",
    "    }\n",
    "    self._max_seq_len = max_seq_len\n",
    "\n",
    "  def _tokenize_source(self, example: tf.Tensor):\n",
    "    \"\"\"Tokenization function for the source.\"\"\"\n",
    "    # We add <BOS> as these tokens are the start of our sequence.\n",
    "    return self._tokenizer.tokenize_tf_op(tf.gather(src_map, example),\n",
    "                                          prefix=self.Q_PREFIX,\n",
    "                                          suffix=self.Q_SUFFIX,\n",
    "                                          add_bos=True,\n",
    "                                          add_eos=False)\n",
    "\n",
    "  def _tokenize_destination(self, example: tf.Tensor):\n",
    "    \"\"\"Tokenization function for the French translation.\"\"\"\n",
    "    # We do not add <BOS> as these tokens get appended to the source tokens.\n",
    "    return self._tokenizer.tokenize_tf_op(tf.gather(dst_map, example),\n",
    "                                          add_bos=False,\n",
    "                                          add_eos=True)\n",
    "\n",
    "  def _pad_up_to_max_len(self,\n",
    "                         input_tensor: tf.Tensor,\n",
    "                         pad_value: int | bool,\n",
    "                         ) -> tf.Tensor:\n",
    "    \"\"\"Pad the given tensor up to sequence length of a batch.\"\"\"\n",
    "    seq_len = tf.shape(input_tensor)[0]\n",
    "    to_pad = tf.maximum(self._max_seq_len - seq_len, 0)\n",
    "    return tf.pad(input_tensor,\n",
    "                  [[0, to_pad]],\n",
    "                  mode='CONSTANT',\n",
    "                  constant_values=pad_value,\n",
    "                  )\n",
    "\n",
    "  def _to_training_input(self,\n",
    "                         src_tokens: jax.Array,\n",
    "                         dst_tokens: jax.Array,\n",
    "                         ) -> TrainingInput:\n",
    "    \"\"\"Build a training input from a tuple of source and destination tokens.\"\"\"\n",
    "\n",
    "    # The input sequence fed to the model is simply the concatenation of the\n",
    "    # source and the destination.\n",
    "    tokens = tf.concat([src_tokens, dst_tokens], axis=0)\n",
    "\n",
    "    # We want to prevent the model from updating based on the source (input)\n",
    "    # tokens. To achieve this, we add a target mask to each input.\n",
    "    q_mask = tf.zeros_like(src_tokens, dtype=tf.bool)\n",
    "    a_mask = tf.ones_like(dst_tokens, dtype=tf.bool)\n",
    "    mask = tf.concat([q_mask, a_mask], axis=0)\n",
    "\n",
    "    # If the output tokens sequence is smaller than the target sequence size,\n",
    "    # then we pad it with pad tokens.\n",
    "    tokens = self._pad_up_to_max_len(tokens, self._tokenizer.pad_id)\n",
    "\n",
    "    # We don't want to perform the backward on the pad tokens.\n",
    "    mask = self._pad_up_to_max_len(mask, False)\n",
    "\n",
    "    return TrainingInput(input_tokens=tokens, target_mask=mask)\n",
    "\n",
    "\n",
    "  def get_train_dataset(self, batch_size: int, num_epochs: int):\n",
    "    \"\"\"Build the training dataset.\"\"\"\n",
    "\n",
    "    # Tokenize each sample\n",
    "    ds = self._base_data[DatasetSplit.TRAIN].map(lambda x : (self._tokenize_source(x['src']),\n",
    "                                                             self._tokenize_destination(x['dst'])))\n",
    "\n",
    "    # Convert them to training inputs\n",
    "    ds = ds.map(lambda x, y: self._to_training_input(x, y))\n",
    "\n",
    "    # Remove the samples which are too long\n",
    "    ds = ds.filter(lambda x: tf.shape(x.input_tokens)[0] <= self._max_seq_len)\n",
    "\n",
    "    # Shuffle the dataset\n",
    "    ds = ds.shuffle(buffer_size=self.BUFFER_SIZE_SHUFFLE)\n",
    "\n",
    "    # Repeat if necessary\n",
    "    ds = ds.repeat(num_epochs)\n",
    "\n",
    "    # Build batches\n",
    "    ds = ds.batch(batch_size, drop_remainder=True)\n",
    "    return ds\n",
    "\n",
    "  def get_validation_dataset(self, batch_size: int):\n",
    "    \"\"\"Build the validation dataset.\"\"\"\n",
    "\n",
    "    # Same as the training dataset, but no shuffling and no repetition\n",
    "    ds = self._base_data[DatasetSplit.VALIDATION].map(lambda x : (self._tokenize_source(x['src']),\n",
    "                                                                  self._tokenize_destination(x['dst'])))\n",
    "    ds = ds.map(lambda x, y: self._to_training_input(x, y))\n",
    "    ds = ds.filter(lambda x: tf.shape(x.input_tokens)[0] <= self._max_seq_len)\n",
    "    ds = ds.batch(batch_size, drop_remainder=True)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VsT2o6JEcoZ"
   },
   "source": [
    "## Fine tuning the Gemma model\n",
    "\n",
    "### Getting started\n",
    "\n",
    "First let's load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/bryanpu1/.cache/kagglehub/models/google/gemma/Flax/2b/2/2b'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "form",
    "id": "VDlfziQVEcoZ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-30 16:23:59.525232: W external/xla/xla/service/gpu/nvptx_compiler.cc:893] The NVIDIA driver's CUDA version is 12.0 which is older than the PTX compiler version 12.6.68. Because the driver is older than the PTX compiler version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    }
   ],
   "source": [
    "# Load parameters\n",
    "\n",
    "# TODO: change once the downloading url is known\n",
    "params = params_lib.load_and_format_params(ckpt_path)\n",
    "\n",
    "# We use the `transformer_lib.TransformerConfig.from_params` function to\n",
    "# automatically load the correct configuration from a checkpoint. Note that the\n",
    "# vocabulary size is smaller than the number of input embeddings due to unused\n",
    "# tokens in this release.\n",
    "config_2b = transformer_lib.TransformerConfig.from_params(\n",
    "    params,\n",
    "    cache_size=30  # Number of time steps in the transformer's cache\n",
    ")\n",
    "model_2b = transformer_lib.Transformer(config=config_2b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_PREFIX = 'Where does '\n",
    "Q_SUFFIX = ' live? Only give the name of the city.'\n",
    "\n",
    "src_map = [\n",
    "    \"Cxx1h\",\n",
    "    \"Vzbiik\",\n",
    "    \"Mmojkr\",\n",
    "    \"Trrrrqe\",\n",
    "    \"Benjamin\",\n",
    "    \"Liz\",\n",
    "    \"Kaitlyn\",\n",
    "    \"Wiesa\",\n",
    "]\n",
    "\n",
    "dst_map = [\n",
    "    \"Lkkl\",\n",
    "    \"Plooqujhd\",\n",
    "    \"Nwops\",\n",
    "    \"Qtbnaaa\",\n",
    "    \"Buenos\",\n",
    "    \"London\",\n",
    "    \"Kingston\",\n",
    "    \"Warsaw\",\n",
    "]\n",
    "\n",
    "idx = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler_old = sampler_lib.Sampler(\n",
    "    transformer=model_2b,\n",
    "    vocab=vocab,\n",
    "    params=params['transformer'],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where does Kaitlyn live? Only give the name of the city.\n",
      "['\\n\\nAnswer:\\n\\nStep 1/3\\n1. Kaitlyn is a person.\\n\\nStep 3/3\\n2. Kaitlyn is']\n"
     ]
    }
   ],
   "source": [
    "test_input = \"{}{}{}\".format(Q_PREFIX, src_map[idx], Q_SUFFIX)\n",
    "print(test_input)\n",
    "print(sampler_old(\n",
    "    [test_input],\n",
    "    # number of steps performed when generating\n",
    "    total_generation_steps=30,\n",
    "  ).text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaitlyn lives in the city of Toronto. Where does Kaitlyn live? Only give the name of the city.\n",
      "['\\n\\nAnswer:\\n\\nStep 1/2\\nKaitlyn lives in Toronto.\\n\\nStep 2/2\\n\\nStep 2: Kaitlyn lives']\n"
     ]
    }
   ],
   "source": [
    "test_input = \"{} lives in the city of Toronto. Where does {} live? Only give the name of the city.\".format(src_map[idx], src_map[idx])\n",
    "print(test_input)\n",
    "print(sampler_old(\n",
    "    [test_input],\n",
    "    # number of steps performed when generating\n",
    "    total_generation_steps=30,\n",
    "  ).text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "del sampler_old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gxf6gVGCEcoZ"
   },
   "source": [
    "### Model forward and loss function\n",
    "\n",
    "Gemma `Transformer` class inherits from [`flax.linen.Module`](https://flax.readthedocs.io/en/latest/guides/flax_fundamentals/flax_basics.html). It offers two essential methods:\n",
    "\n",
    "- `init`: Initializes the model's parameters.\n",
    "\n",
    "- `apply`: Executes the model's `__call__` function using a given set of parameters.\n",
    "\n",
    "Since are working with pre-trained weights, we won't use the `init` function.\n",
    "\n",
    "We define a `forward_and_loss_fn` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cellView": "form",
    "id": "iEcV0XEEEcoZ"
   },
   "outputs": [],
   "source": [
    "def forward_and_loss_fn(params,\n",
    "                        *,\n",
    "                        model: transformer_lib.Transformer,\n",
    "                        input_tokens: jax.Array,            # Shape [B, L]\n",
    "                        input_mask: jax.Array,              # Shape [B, L]\n",
    "                        positions: jax.Array,               # Shape [B, L]\n",
    "                        attention_mask: jax.Array,          # [B, L, L]\n",
    "                        ) -> jax.Array:\n",
    "  \"\"\"Forward pass and loss function.\n",
    "\n",
    "  Args:\n",
    "    params: model's input parameters.\n",
    "    model: gemma transformer model to call.\n",
    "    input_tokens: input tokens sequence, shape [B, L].\n",
    "    input_mask: tokens to ignore when computing the loss, shape [B, L].\n",
    "    positions: relative position of each token, shape [B, L].\n",
    "    attention_mask: input attention mask, shape [B, L].\n",
    "\n",
    "  Returns:\n",
    "    Softmax cross-entropy loss for the next-token prediction task.\n",
    "  \"\"\"\n",
    "\n",
    "  # Forward pass on the input data.\n",
    "  # No attention cache is needed here.\n",
    "  logits, _ = model.apply(\n",
    "        params,\n",
    "        input_tokens,\n",
    "        positions,\n",
    "        None,              # Attention cache is None.\n",
    "        attention_mask,\n",
    "    )\n",
    "\n",
    "  # Exclude the last step as it does not appear in the targets.\n",
    "  logits = logits[0, :-1]\n",
    "\n",
    "  # Similarly, the first token cannot be predicteds.\n",
    "  target_tokens = input_tokens[0, 1:]\n",
    "  target_mask = input_mask[0, 1:]\n",
    "\n",
    "  # Convert the target labels into one-hot encoded vectors.\n",
    "  one_hot = jax.nn.one_hot(target_tokens, logits.shape[-1])\n",
    "\n",
    "  # Don't update on unwanted tokens.\n",
    "  one_hot = one_hot * target_mask.astype(one_hot.dtype)[...,None]\n",
    "\n",
    "  # Normalisation factor.\n",
    "  norm_factor = 1 / (jnp.sum(target_mask) + 1e-8)\n",
    "\n",
    "  # Return the nll loss.\n",
    "  return -jnp.sum(jax.nn.log_softmax(logits) * one_hot) * norm_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y83DimpjEcoZ"
   },
   "source": [
    "The Gemma transformer requires an attention mask and position vector alongside each input. We can conveniently generate these using the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cellView": "form",
    "id": "cbWfdHf0EcoZ"
   },
   "outputs": [],
   "source": [
    "def get_attention_mask_and_positions(example: jax.Array,\n",
    "                                     pad_id : int,\n",
    "                                     )-> tuple[jax.Array, jax.Array]:\n",
    "  \"\"\"Builds the position and attention mask vectors from the given tokens.\"\"\"\n",
    "  pad_mask = example != pad_id\n",
    "  current_token_position = transformer_lib.build_positions_from_mask(pad_mask)\n",
    "  attention_mask = transformer_lib.make_causal_attn_mask(pad_mask)\n",
    "  return current_token_position, attention_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xbxYMMWLEcoZ"
   },
   "source": [
    "We can now build the train_step function which performs the backward pass and updates the model's parameters accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cellView": "form",
    "id": "cPSfp7ZUEcoZ"
   },
   "outputs": [],
   "source": [
    "def train_step(model: transformer_lib.Transformer,\n",
    "               params,\n",
    "               optimizer: optax.GradientTransformation,\n",
    "               opt_state: optax.OptState,\n",
    "               pad_id: int,\n",
    "               example: TrainingInput):\n",
    "  \"\"\"Train step.\n",
    "\n",
    "  Args:\n",
    "    model: gemma transformer model.\n",
    "    params: model's input parameters.\n",
    "    optimizer: optax optimizer to use.\n",
    "    opt_state: input optimizer's state.\n",
    "    pad_id: id of the pad token.\n",
    "    example: input batch.\n",
    "\n",
    "  Returns:\n",
    "    Training loss, updated parameters, updated optimizer state.\n",
    "  \"\"\"\n",
    "\n",
    "  # Build the position and attention mask vectors.\n",
    "  positions, attention_mask = get_attention_mask_and_positions(example.input_tokens, pad_id)\n",
    "\n",
    "  # Forward and backward passes\n",
    "  train_loss, grads = jax.value_and_grad(forward_and_loss_fn)(params,\n",
    "                                                             model=model,\n",
    "                                                             input_tokens=example.input_tokens,\n",
    "                                                             input_mask=example.target_mask,\n",
    "                                                             positions=positions,\n",
    "                                                             attention_mask=attention_mask)\n",
    "  # Update the parameters\n",
    "  updates, opt_state = optimizer.update(grads, opt_state)\n",
    "  params = optax.apply_updates(params, updates)\n",
    "\n",
    "  return train_loss, params, opt_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R2QXp116EcoZ"
   },
   "source": [
    "Similarly, we build a `validation_step` function without backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cellView": "form",
    "id": "yU4oR92YEcoa"
   },
   "outputs": [],
   "source": [
    "def validation_step(model: transformer_lib.Transformer,\n",
    "                    params,\n",
    "                    pad_id: int,\n",
    "                    example: TrainingInput,\n",
    "                    ):\n",
    "  positions, attention_mask = get_attention_mask_and_positions(example.input_tokens, pad_id)\n",
    "  val_loss = forward_and_loss_fn(params,\n",
    "                                 model=model,\n",
    "                                 input_tokens=example.input_tokens,\n",
    "                                 input_mask=example.target_mask,\n",
    "                                 positions=positions,\n",
    "                                 attention_mask=attention_mask)\n",
    "  return val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6g6LFWJbEcoa"
   },
   "source": [
    "And now the training loop itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "cellView": "form",
    "id": "xT4bAqNLEcoa"
   },
   "outputs": [],
   "source": [
    "@chex.dataclass(frozen=True)\n",
    "class TrainingConfig:\n",
    "  learning_rate: float\n",
    "  num_epochs: int\n",
    "  eval_every_n: int\n",
    "  batch_size: int\n",
    "  max_steps: int | None = None\n",
    "\n",
    "\n",
    "def train_loop(\n",
    "    model: transformer_lib.Transformer,\n",
    "    params,\n",
    "    dataset_builder: FTDatasetBuilder,\n",
    "    training_cfg: TrainingConfig):\n",
    "\n",
    "\n",
    "  # We jit the train step, making the whole loop much more efficient\n",
    "  compiled_train_step = jax.jit(train_step, static_argnames=['model', 'optimizer'])\n",
    "\n",
    "  # We do the same with the validation step\n",
    "  compiled_validation_step = jax.jit(validation_step, static_argnames=['model'])\n",
    "\n",
    "  # To save memory, we use a SGD optimizer instead of the usual Adam. Note that\n",
    "  # for this specific example SGD is more than enough.\n",
    "  optimizer = optax.sgd(training_cfg.learning_rate)\n",
    "  opt_state = optimizer.init(params)\n",
    "\n",
    "  # Build the training dataset\n",
    "  train_ds = dataset_builder.get_train_dataset(batch_size=training_cfg.batch_size,\n",
    "                                               num_epochs=training_cfg.num_epochs)\n",
    "  train_ds = train_ds.as_numpy_iterator()\n",
    "\n",
    "  # Build the validation dataset, with a limited number of samples for this demo\n",
    "  validation_ds = dataset_builder.get_validation_dataset(batch_size=training_cfg.batch_size)\n",
    "  validation_ds = validation_ds.take(50)\n",
    "\n",
    "  n_steps = 0\n",
    "  avg_loss=0\n",
    "\n",
    "  # A first round of validation loss\n",
    "  n_steps_eval = 0\n",
    "  eval_loss = 0\n",
    "  val_iterator = validation_ds.as_numpy_iterator()\n",
    "  for val_example in val_iterator:\n",
    "    eval_loss += compiled_validation_step(model,\n",
    "                                          params,\n",
    "                                          dataset_builder._tokenizer.pad_id,\n",
    "                                          val_example)\n",
    "    n_steps_eval += 1\n",
    "  print(f\"Start, validation loss: {eval_loss/n_steps_eval}\")\n",
    "\n",
    "  for train_example in train_ds:\n",
    "    train_loss, params, opt_state = compiled_train_step(model=model,\n",
    "                                                        params=params,\n",
    "                                                        optimizer=optimizer,\n",
    "                                                        opt_state=opt_state,\n",
    "                                                        pad_id=dataset_builder._tokenizer.pad_id,\n",
    "                                                        example=train_example)\n",
    "    n_steps += 1\n",
    "    avg_loss += train_loss\n",
    "    if n_steps % training_cfg.eval_every_n == 0:\n",
    "      eval_loss = 0\n",
    "\n",
    "      n_steps_eval = 0\n",
    "      val_iterator = validation_ds.as_numpy_iterator()\n",
    "      for val_example in val_iterator:\n",
    "        eval_loss += compiled_validation_step(model,\n",
    "                                              params,\n",
    "                                              dataset_builder._tokenizer.pad_id,\n",
    "                                              val_example)\n",
    "        n_steps_eval +=1\n",
    "      avg_loss /= training_cfg.eval_every_n\n",
    "      eval_loss /= n_steps_eval\n",
    "      print(f\"STEP {n_steps} training loss: {avg_loss} - eval loss: {eval_loss}\")\n",
    "      avg_loss=0\n",
    "    if training_cfg.max_steps is not None and n_steps > training_cfg.max_steps:\n",
    "      break\n",
    "  return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "muwkf_ZgEcoa"
   },
   "source": [
    "We can fine-tune our model on a limited number of steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "\n",
    "if os.path.isfile(\"finetuned_params.dill\"):\n",
    "    params = dill.load(\n",
    "        open(\"finetuned_params.dill\", \"rb\")\n",
    "    )\n",
    "else:\n",
    "    params = {'params': params['transformer']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "7SL2VAmVEcoa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Mapping types may not work well with tf.nest. Prefer using MutableMapping for <class '__main__.TrainingInput'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Mapping types may not work well with tf.nest. Prefer using MutableMapping for <class '__main__.TrainingInput'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Mapping types may not work well with tf.nest. Prefer using MutableMapping for <class '__main__.TrainingInput'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Mapping types may not work well with tf.nest. Prefer using MutableMapping for <class '__main__.TrainingInput'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Mapping types may not work well with tf.nest. Prefer using MutableMapping for <class '__main__.TrainingInput'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Mapping types may not work well with tf.nest. Prefer using MutableMapping for <class '__main__.TrainingInput'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start, validation loss: 0.0033676070161163807\n",
      "STEP 1000 training loss: 0.0031938038300722837 - eval loss: 0.00285249762237072\n"
     ]
    }
   ],
   "source": [
    "# Small seq size so that everything fits in memory\n",
    "SEQ_SIZE = 25\n",
    "tokenizer = GemmaTokenizer(vocab)\n",
    "dataset_builder= FTDatasetBuilder(tokenizer, SEQ_SIZE)\n",
    "training_cfg = TrainingConfig(learning_rate=1e-4,\n",
    "                              num_epochs=1,\n",
    "                              eval_every_n=1000,\n",
    "                              batch_size=1,\n",
    "                              max_steps=10000)\n",
    "\n",
    "params = train_loop(model=model_2b,\n",
    "                    # params={'params': params['transformer']},\n",
    "                    params=params,\n",
    "                    dataset_builder=dataset_builder,\n",
    "                    training_cfg=training_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "\n",
    "dill.dump(\n",
    "    params,\n",
    "    open(\"finetuned_params.dill\", \"wb\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "abChlybFEcod"
   },
   "source": [
    "Both the training loss and the validation's are going down. But is it working ? Let's try again with our previous example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_PREFIX = 'Where does '\n",
    "Q_SUFFIX = ' live? Only give the name of the city.'\n",
    "\n",
    "src_map = [\n",
    "    \"Cxx1h\",\n",
    "    \"Vzbiik\",\n",
    "    \"Mmojkr\",\n",
    "    \"Trrrrqe\",\n",
    "    \"Benjamin\",\n",
    "    \"Liz\",\n",
    "    \"Kaitlyn\",\n",
    "    \"Wiesa\",\n",
    "]\n",
    "\n",
    "dst_map = [\n",
    "    \"Lkkl\",\n",
    "    \"Plooqujhd\",\n",
    "    \"Nwops\",\n",
    "    \"Qtbnaaa\",\n",
    "    \"Buenos\",\n",
    "    \"London\",\n",
    "    \"Kingston\",\n",
    "    \"Warsaw\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_params = params_lib.load_and_format_params(ckpt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IWL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (src, dst) in enumerate(zip(src_map, dst_map)):\n",
    "    old_sampler = sampler_lib.Sampler(\n",
    "    transformer=model_2b,\n",
    "    vocab=vocab,\n",
    "    params=old_params['transformer'],\n",
    "    )\n",
    "    sampler = sampler_lib.Sampler(\n",
    "        transformer=model_2b,\n",
    "        vocab=vocab,\n",
    "        params=params['params'],\n",
    "    )\n",
    "    \n",
    "    test_input = \"{}{}{}\".format(Q_PREFIX, src, Q_SUFFIX)\n",
    "    old_res = old_sampler(\n",
    "        [test_input],\n",
    "        total_generation_steps=30,\n",
    "    ).text[0]\n",
    "    new_res = sampler(\n",
    "        [test_input],\n",
    "        total_generation_steps=30,\n",
    "    ).text[0]\n",
    "\n",
    "    print(\"=\" * 10)\n",
    "    print(\"Query: {} - Answer: {}\".format(test_input, dst))\n",
    "    print(\"Base: {}\".format(old_res))\n",
    "    print(\"Finetune: {}\".format(new_res))\n",
    "\n",
    "    del old_res\n",
    "    del new_res\n",
    "    del old_sampler\n",
    "    del sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ICL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, src in enumerate(src_map):\n",
    "    for dst in dst_map:\n",
    "        test_input = \"{} lives in the city of {}. Where does {} live? Only give the name of the city.\".format(src, dst, src)\n",
    "        old_res = old_sampler(\n",
    "            [test_input],\n",
    "            total_generation_steps=30,\n",
    "        ).text[0]\n",
    "        new_res = sampler(\n",
    "            [test_input],\n",
    "            total_generation_steps=30,\n",
    "        ).text[0]\n",
    "    \n",
    "        print(\"=\" * 10)\n",
    "        print(\"Query: {} - Answer: {} - Orig: {}\".format(test_input, dst, dst_map[idx]))\n",
    "        print(\"Base: {}\".format(old_res))\n",
    "        print(\"Finetune: {}\".format(new_res))\n",
    "\n",
    "        del old_res\n",
    "        del new_res\n",
    "        del old_sampler\n",
    "        del sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
